import torch
import os
import json
import folder_paths
from PIL import Image
import numpy as np
from transformers import AutoModelForVision2Seq, AutoProcessor
from transformers import BitsAndBytesConfig
from huggingface_hub import snapshot_download
import gc


class Qwen3VLImageCaption:
    """Qwen3-VLå›¾åƒæè¿°èŠ‚ç‚¹ - ä¸“é—¨ç”¨äºç”Ÿæˆå›¾åƒæè¿°"""
    
    # æç¤ºé¢„è®¾å­—å…¸
    CAPTION_PRESETS = {
        "æç¤ºé£æ ¼ - æ ‡ç­¾": "ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®å›¾åƒä¸­çš„è§†è§‰ä¿¡æ¯ï¼Œä¸ºæ–‡æœ¬åˆ°å›¾åƒAIç”Ÿæˆä¸€ä¸ªç®€æ´çš„é€—å·åˆ†éš”æ ‡ç­¾åˆ—è¡¨ã€‚å°†è¾“å‡ºé™åˆ¶åœ¨æœ€å¤š50ä¸ªç‹¬ç‰¹æ ‡ç­¾ã€‚ä¸¥æ ¼æè¿°è§†è§‰å…ƒç´ ï¼Œå¦‚ä¸»ä½“ã€æœè£…ã€ç¯å¢ƒã€é¢œè‰²ã€å…‰çº¿å’Œæ„å›¾ã€‚ä¸è¦åŒ…å«æŠ½è±¡æ¦‚å¿µã€è§£é‡Šã€è¥é”€æœ¯è¯­æˆ–æŠ€æœ¯æœ¯è¯­ï¼ˆä¾‹å¦‚ï¼Œä¸è¦åŒ…å«'SEO'ã€'å“ç‰Œå¯¹é½'ã€'ç—…æ¯’å¼ä¼ æ’­æ½œåŠ›'ï¼‰ã€‚ç›®æ ‡æ˜¯ç®€æ´çš„è§†è§‰æè¿°ç¬¦åˆ—è¡¨ã€‚é¿å…é‡å¤æ ‡ç­¾ã€‚",
        "æç¤ºé£æ ¼ - ç®€æ´": "åˆ†æå›¾åƒå¹¶ç”Ÿæˆä¸€ä¸ªç®€å•çš„å•å¥æ–‡æœ¬åˆ°å›¾åƒæç¤ºã€‚ç®€æ´åœ°æè¿°ä¸»è¦ä¸»ä½“å’Œç¯å¢ƒã€‚",
        "æç¤ºé£æ ¼ - è¯¦ç»†": "åŸºäºå›¾åƒç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„ã€è‰ºæœ¯æ€§çš„æ–‡æœ¬åˆ°å›¾åƒæç¤ºã€‚å°†ä¸»ä½“ã€åŠ¨ä½œã€ç¯å¢ƒã€å…‰çº¿å’Œæ•´ä½“æ°›å›´ç»“åˆæˆä¸€ä¸ªè¿è´¯çš„æ®µè½ï¼Œå¤§çº¦2-3å¥è¯ã€‚ä¸“æ³¨äºå…³é”®çš„è§†è§‰ç»†èŠ‚ã€‚",
        "æç¤ºé£æ ¼ - æè¯¦ç»†": "ä»å›¾åƒç”Ÿæˆä¸€ä¸ªæå…¶è¯¦ç»†å’Œæè¿°æ€§çš„æ–‡æœ¬åˆ°å›¾åƒæç¤ºã€‚åˆ›å»ºä¸€ä¸ªä¸°å¯Œçš„æ®µè½ï¼Œè¯¦ç»†é˜è¿°ä¸»ä½“çš„å¤–è§‚ã€æœè£…çº¹ç†ã€ç‰¹å®šçš„èƒŒæ™¯å…ƒç´ ã€å…‰çº¿çš„è´¨é‡å’Œé¢œè‰²ã€é˜´å½±å’Œæ•´ä½“æ°›å›´ã€‚ç›®æ ‡æ˜¯é«˜åº¦æè¿°æ€§å’Œæ²‰æµ¸å¼çš„æç¤ºã€‚",
        "æç¤ºé£æ ¼ - ç”µå½±æ„Ÿ": "ä½œä¸ºå¤§å¸ˆçº§æç¤ºå·¥ç¨‹å¸ˆã€‚ä¸ºå›¾åƒç”ŸæˆAIåˆ›å»ºä¸€ä¸ªé«˜åº¦è¯¦ç»†å’Œå¯Œæœ‰è¡¨ç°åŠ›çš„æç¤ºã€‚æè¿°ä¸»ä½“ã€å§¿åŠ¿ã€ç¯å¢ƒã€å…‰çº¿ã€æ°›å›´å’Œè‰ºæœ¯é£æ ¼ï¼ˆä¾‹å¦‚ï¼Œç…§ç‰‡çº§çœŸå®ã€ç”µå½±æ„Ÿã€ç»˜ç”»é£æ ¼ï¼‰ã€‚å°†æ‰€æœ‰å…ƒç´ ç¼–ç»‡æˆä¸€ä¸ªè‡ªç„¶çš„è¯­è¨€æ®µè½ï¼Œä¸“æ³¨äºè§†è§‰å†²å‡»ã€‚",
        "åˆ›æ„ - è¯¦ç»†åˆ†æ": "è¯¦ç»†æè¿°è¿™å¼ å›¾åƒï¼Œå°†ä¸»ä½“ã€æœè£…ã€é…é¥°ã€èƒŒæ™¯å’Œæ„å›¾åˆ†è§£ä¸ºç‹¬ç«‹çš„éƒ¨åˆ†ã€‚",
        "åˆ›æ„ - è§†é¢‘æ€»ç»“": "æ€»ç»“è¿™ä¸ªè§†é¢‘ä¸­çš„å…³é”®äº‹ä»¶å’Œå™äº‹è¦ç‚¹ã€‚",
        "åˆ›æ„ - çŸ­ç¯‡æ•…äº‹": "æ ¹æ®è¿™å¼ å›¾åƒæˆ–è§†é¢‘å†™ä¸€ä¸ªå¯Œæœ‰æƒ³è±¡åŠ›çš„çŸ­ç¯‡æ•…äº‹ã€‚",
        "åˆ›æ„ - ä¼˜åŒ–æ‰©å±•æç¤º": "ä¼˜åŒ–å’Œå¢å¼ºä»¥ä¸‹ç”¨æˆ·æç¤ºï¼Œç”¨äºåˆ›æ„æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆã€‚ä¿æŒå«ä¹‰å’Œå…³é”®è¯ï¼Œä½¿å…¶æ›´å…·è¡¨ç°åŠ›å’Œè§†è§‰ä¸°å¯Œæ€§ã€‚ä»…è¾“å‡ºæ”¹è¿›åçš„æç¤ºæ–‡æœ¬æœ¬èº«ï¼Œä¸è¦æœ‰ä»»ä½•æ¨ç†æ­¥éª¤ã€æ€è€ƒè¿‡ç¨‹æˆ–é¢å¤–è¯„è®ºã€‚"
    }
    
    def __init__(self):
        self.model = None
        self.processor = None
        self.current_model_name = None
        
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": ("IMAGE",),
                "caption_prompt": ("STRING", {
                    "default": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡ï¼ŒåŒ…æ‹¬ä¸»è¦å¯¹è±¡ã€åœºæ™¯ã€é¢œè‰²ã€é£æ ¼å’Œå…¶ä»–ç‰¹å¾ã€‚",
                    "multiline": True
                }),
                "preset_selection": (["æ— é¢„è®¾"] + list(cls.CAPTION_PRESETS.keys()), {
                    "default": "æ— é¢„è®¾"
                }),
                "system_prompt": ("STRING", {
                    "default": "æ‚¨æ˜¯ä¸€ä½ä¸“ä¸šçš„å›¾åƒåˆ†æåŠ©æ‰‹ï¼Œèƒ½å¤Ÿå‡†ç¡®æè¿°å›¾åƒä¸­çš„å„ç§ç»†èŠ‚ã€‚",
                    "multiline": True
                }),
                "max_tokens": ("INT", {
                    "default": 512,
                    "min": 50,
                    "max": 2048,
                    "step": 50
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.1,
                    "max": 2.0,
                    "step": 0.1
                }),

            },
            "optional": {
                "model_name": (cls.get_available_models(), {
                    "default": "Qwen3-VL-2B-Instruct"
                }),
                "device": (["Auto", "cuda", "cpu", "mps"], {
                    "default": "Auto"
                }),
                "quantization": (["æ— ï¼ˆFP16ï¼‰", "4-bit", "8-bit"], {
                    "default": "4-bit"
                }),
                "attention_type": (["Eager: æœ€ä½³å…¼å®¹æ€§", "SDPA: å¹³è¡¡", "Flash Attention 2: æœ€ä½³æ€§èƒ½"], {
                    "default": "Eager: æœ€ä½³å…¼å®¹æ€§"
                }),
                "seed": ("INT", {
                    "default": -1,
                    "min": -1,
                    "max": 0xffffffffffffffff,
                    "description": "éšæœºç§å­ï¼Œ-1è¡¨ç¤ºéšæœº"
                }),
                "clear_cache": ("BOOLEAN", {
                    "default": False
                }),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("å›¾åƒæè¿°", "è¯¦ç»†å“åº”")
    FUNCTION = "generate_caption"
    CATEGORY = "QwenVL-ALL"
    
    # åŸºç¡€æ¨¡å‹æ˜ å°„
    BASE_MODEL_REPO_MAP = {
        "Qwen3-VL-2B-Instruct": "Qwen/Qwen3-VL-2B-Instruct",
        "Qwen3-VL-2B-Thinking": "Qwen/Qwen3-VL-2B-Thinking",
        "Qwen3-VL-2B-Instruct-FP8": "Qwen/Qwen3-VL-2B-Instruct-FP8",
        "Qwen3-VL-2B-Thinking-FP8": "Qwen/Qwen3-VL-2B-Thinking-FP8",
        "Qwen3-VL-4B-Instruct": "Qwen/Qwen3-VL-4B-Instruct",
        "Qwen3-VL-4B-Thinking": "Qwen/Qwen3-VL-4B-Thinking",
        "Qwen3-VL-4B-Instruct-FP8": "Qwen/Qwen3-VL-4B-Instruct-FP8",
        "Qwen3-VL-4B-Thinking-FP8": "Qwen/Qwen3-VL-4B-Thinking-FP8",
        "Qwen3-VL-8B-Instruct": "Qwen/Qwen3-VL-8B-Instruct",
        "Qwen3-VL-8B-Thinking": "Qwen/Qwen3-VL-8B-Thinking",
        "Qwen3-VL-8B-Instruct-FP8": "Qwen/Qwen3-VL-8B-Instruct-FP8",
        "Qwen3-VL-8B-Thinking-FP8": "Qwen/Qwen3-VL-8B-Thinking-FP8"
    }
    
    @classmethod
    def get_available_models(cls):
        """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨ï¼ŒåŒ…æ‹¬åŸºç¡€æ¨¡å‹å’Œæœ¬åœ°æ¨¡å‹æ–‡ä»¶å¤¹ä¸­çš„æ¨¡å‹"""
        # è·å–åŸºç¡€æ¨¡å‹åˆ—è¡¨ï¼Œå¹¶æ£€æŸ¥æœ¬åœ°æ˜¯å¦å·²ä¸‹è½½
        available_models = []
        models_dir = os.path.join(folder_paths.models_dir, "LLM", "Qwen-VL")
        
        # æ·»åŠ åŸºç¡€æ¨¡å‹ï¼Œæ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
        for model_name in cls.BASE_MODEL_REPO_MAP.keys():
            # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å­˜åœ¨è¯¥æ¨¡å‹
            model_path = os.path.join(models_dir, model_name)
            if os.path.exists(model_path) and os.path.isdir(model_path):
                available_models.append(f"{model_name}ï¼ˆå·²ä¸‹è½½ï¼‰")
            else:
                available_models.append(model_name)
        
        # æ‰«ææœ¬åœ°æ¨¡å‹æ–‡ä»¶å¤¹ï¼Œæ·»åŠ ç”¨æˆ·è‡ªå®šä¹‰çš„æœ¬åœ°æ¨¡å‹
        if os.path.exists(models_dir):
            for item in os.listdir(models_dir):
                item_path = os.path.join(models_dir, item)
                # æ£€æŸ¥æ˜¯å¦ä¸ºç›®å½•ä¸”ä¸åœ¨åŸºç¡€æ¨¡å‹åˆ—è¡¨ä¸­
                if os.path.isdir(item_path):
                    # æå–æ¨¡å‹åç§°ï¼ˆå»é™¤"ï¼ˆå·²ä¸‹è½½ï¼‰"æ ‡è®°ï¼‰
                    clean_model_name = item.replace("ï¼ˆå·²ä¸‹è½½ï¼‰", "")
                    if clean_model_name not in cls.BASE_MODEL_REPO_MAP.keys():
                        # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨åˆ—è¡¨ä¸­ï¼ˆé¿å…é‡å¤ï¼‰
                        display_name = f"{item}ï¼ˆå·²ä¸‹è½½ï¼‰" if "ï¼ˆå·²ä¸‹è½½ï¼‰" not in item else item
                        if display_name not in available_models:
                            available_models.append(display_name)
        
        return available_models
    
    # åŠ¨æ€æ¨¡å‹æ˜ å°„ï¼Œç»“åˆåŸºç¡€æ¨¡å‹å’Œæœ¬åœ°æ¨¡å‹
    @classmethod
    def get_model_repo_map(cls):
        """è·å–æ¨¡å‹ä»“åº“æ˜ å°„ï¼ŒåŒ…æ‹¬åŸºç¡€æ¨¡å‹å’Œæœ¬åœ°æ¨¡å‹"""
        model_repo_map = cls.BASE_MODEL_REPO_MAP.copy()
        
        # æ‰«ææœ¬åœ°æ¨¡å‹æ–‡ä»¶å¤¹
        models_dir = os.path.join(folder_paths.models_dir, "LLM", "Qwen-VL")
        if os.path.exists(models_dir):
            for item in os.listdir(models_dir):
                item_path = os.path.join(models_dir, item)
                # æ£€æŸ¥æ˜¯å¦ä¸ºç›®å½•
                if os.path.isdir(item_path):
                    # æå–å¹²å‡€çš„æ¨¡å‹åç§°ï¼ˆå»é™¤"ï¼ˆå·²ä¸‹è½½ï¼‰"æ ‡è®°ï¼‰
                    clean_model_name = item.replace("ï¼ˆå·²ä¸‹è½½ï¼‰", "")
                    # å¦‚æœå¹²å‡€çš„æ¨¡å‹åç§°ä¸åœ¨æ˜ å°„ä¸­ï¼Œåˆ™æ·»åŠ åˆ°æ˜ å°„ä¸­
                    if clean_model_name not in model_repo_map:
                        # å¯¹äºæœ¬åœ°æ¨¡å‹ï¼Œä½¿ç”¨ç›®å½•åä½œä¸ºä»“åº“ID
                        model_repo_map[clean_model_name] = clean_model_name
        
        return model_repo_map
    
    def get_device(self, device_preference):
        """è·å–è®¾å¤‡"""
        if device_preference == "Auto":
            if torch.cuda.is_available():
                return "cuda"
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                return "mps"
            else:
                return "cpu"
        return device_preference.lower()
    
    def _download_model_with_progress(self, model_id: str, local_dir: str):
        """ä¸‹è½½æ¨¡å‹å¹¶æ˜¾ç¤ºè¿›åº¦"""
        print(f"\n{'='*70}")
        print(f"[Qwen3-VL Image Caption] ğŸ“¥ å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_id}")
        print(f"[Qwen3-VL Image Caption] ğŸ“‚ ä¿å­˜è·¯å¾„: {local_dir}")
        print(f"[Qwen3-VL Image Caption] â³ è¯·è€å¿ƒç­‰å¾…ï¼Œä¸‹è½½å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿåˆ°å‡ ååˆ†é’Ÿ...")
        print(f"[Qwen3-VL Image Caption] ğŸ’¡ ä¸‹è½½è¿›åº¦å°†åœ¨ä¸‹æ–¹æ˜¾ç¤º")
        print(f"{'='*70}\n")
        
        try:
            # snapshot_download å°†è‡ªåŠ¨ä¸ºæ¯ä¸ªæ–‡ä»¶æ˜¾ç¤ºä¸‹è½½è¿›åº¦æ¡
            snapshot_download(
                repo_id=model_id,
                local_dir=local_dir,
                local_dir_use_symlinks=False,
                resume_download=True,
            )
            print(f"\n{'='*70}")
            print(f"[Qwen3-VL Image Caption] âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {model_id}")
            print(f"{'='*70}\n")
        except Exception as e:
            print(f"\n{'='*70}")
            print(f"[Qwen3-VL Image Caption] âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
            print(f"[Qwen3-VL Image Caption] ğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
            print(f"[Qwen3-VL Image Caption]    1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print(f"[Qwen3-VL Image Caption]    2. ä½¿ç”¨é•œåƒç«™: export HF_ENDPOINT=https://hf-mirror.com")
            print(f"[Qwen3-VL Image Caption]    3. ä½¿ç”¨ä»£ç†: export HTTP_PROXY=http://127.0.0.1:7890")
            print(f"[Qwen3-VL Image Caption]    4. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°: {local_dir}")
            print(f"{'='*70}\n")
            raise
    
    def load_model(self, model_name, device, quantization="æ— ï¼ˆFP16ï¼‰", attention_type="Eager: æœ€ä½³å…¼å®¹æ€§"):
        """åŠ è½½æ¨¡å‹"""
        # æå–å¹²å‡€çš„æ¨¡å‹åç§°ï¼ˆå»é™¤"ï¼ˆå·²ä¸‹è½½ï¼‰"æ ‡è®°ï¼‰
        clean_model_name = model_name.replace("ï¼ˆå·²ä¸‹è½½ï¼‰", "")
        
        if self.current_model_name == clean_model_name and self.model is not None:
            return
            
        # æ¸…ç†ä¹‹å‰çš„æ¨¡å‹
        self.cleanup_model()
        
        # è·å–æ¨¡å‹æ˜ å°„
        model_repo_map = self.get_model_repo_map()
        
        # ä»æ˜ å°„ä¸­è·å–ä»“åº“IDï¼Œä½¿ç”¨å¹²å‡€çš„æ¨¡å‹åç§°
        repo_id = model_repo_map.get(clean_model_name)
        if not repo_id:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {clean_model_name}")
        
        # ä½¿ç”¨å¹²å‡€çš„æ¨¡å‹åç§°ä½œä¸ºæœ¬åœ°ç›®å½•å
        model_checkpoint = os.path.join(
            folder_paths.models_dir, "LLM", "Qwen-VL", clean_model_name
        )
        
        # å¯¹äºHuggingFaceæ¨¡å‹ï¼ˆåŒ…å«"/"ï¼‰ï¼Œéœ€è¦ä¸‹è½½
        if "/" in repo_id:
            # å¦‚æœæ¨¡å‹ä¸å­˜åœ¨åˆ™ä¸‹è½½
            if not os.path.exists(model_checkpoint):
                self._download_model_with_progress(repo_id, model_checkpoint)
        else:
            # å¯¹äºæœ¬åœ°æ¨¡å‹ï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨
            if not os.path.exists(model_checkpoint):
                raise ValueError(f"æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨: {model_checkpoint}")
        
        try:
            print(f"[Qwen3-VL Image Caption] æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name} åˆ°è®¾å¤‡: {device}")
            
            # è®¾ç½®æ¨¡å‹åŠ è½½å‚æ•°
            model_kwargs = {
                "dtype": torch.float16 if device == "cuda" else torch.float32,
                "device_map": "auto" if device == "cuda" else None,
                "low_cpu_mem_usage": True,
                "trust_remote_code": True
            }
            
            # é…ç½®é‡åŒ–
            quantization_config = None
            if quantization == "4-bit":
                quantization_config = BitsAndBytesConfig(load_in_4bit=True)
                model_kwargs["quantization_config"] = quantization_config
            elif quantization == "8-bit":
                quantization_config = BitsAndBytesConfig(load_in_8bit=True)
                model_kwargs["quantization_config"] = quantization_config
            
            # é…ç½®æ³¨æ„åŠ›ç±»å‹
            attention_type_map = {
                "Eager: æœ€ä½³å…¼å®¹æ€§": "eager",
                "SDPA: å¹³è¡¡": "sdpa", 
                "Flash Attention 2: æœ€ä½³æ€§èƒ½": "flash_attention_2"
            }
            attn_implementation = attention_type_map.get(attention_type, "eager")
            model_kwargs["attn_implementation"] = attn_implementation
            
            # ä»æœ¬åœ°ç›®å½•åŠ è½½å¤„ç†å™¨å’Œæ¨¡å‹
            self.processor = AutoProcessor.from_pretrained(
                model_checkpoint,
                trust_remote_code=True
            )
            
            self.model = AutoModelForVision2Seq.from_pretrained(
                model_checkpoint,
                **model_kwargs
            )
            
            if device != "cuda":
                self.model = self.model.to(device)
                
            self.current_model_name = model_name
            print(f"[Qwen3-VL Image Caption] æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
            
        except Exception as e:
            print(f"[Qwen3-VL Image Caption] æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}ã€‚è¯·ç¡®ä¿ç½‘ç»œè¿æ¥æˆ–æ‰‹åŠ¨å°†æ¨¡å‹ä¸‹è½½åˆ° ComfyUI/models/LLM/Qwen-VL/ ç›®å½•")
    
    def cleanup_model(self):
        """æ¸…ç†æ¨¡å‹ç¼“å­˜"""
        if self.model is not None:
            del self.model
            self.model = None
        if self.processor is not None:
            del self.processor
            self.processor = None
        self.current_model_name = None
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def tensor_to_pil(self, image_tensor):
        """å°†Tensorè½¬æ¢ä¸ºPILå›¾åƒ"""
        tensor = image_tensor
        if tensor.dim() == 4:
            tensor = tensor.squeeze(0)
        if tensor.dim() == 3 and tensor.shape[2] == 3:
            tensor = tensor.permute(2, 0, 1)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        if tensor.max() <= 1.0:
            tensor = tensor * 255.0
        
        tensor = tensor.clamp(0, 255)
        array = tensor.cpu().numpy().astype(np.uint8)
        
        # ç¡®ä¿æ­£ç¡®çš„å½¢çŠ¶
        if array.shape[0] == 3:
            array = array.transpose(1, 2, 0)
        
        return Image.fromarray(array)
    
    def generate_caption(self, image, caption_prompt, preset_selection, system_prompt, max_tokens, temperature, seed=-1, model_name="Qwen3-VL-2B-Instruct", device="Auto", quantization="æ— ï¼ˆFP16ï¼‰", attention_type="Eager: æœ€ä½³å…¼å®¹æ€§", clear_cache=False):
        """ç”Ÿæˆå›¾åƒæè¿°"""
        
        # æå–å¹²å‡€çš„æ¨¡å‹åç§°ï¼ˆå»é™¤"ï¼ˆå·²ä¸‹è½½ï¼‰"æ ‡è®°ï¼‰
        clean_model_name = model_name.replace("ï¼ˆå·²ä¸‹è½½ï¼‰", "")
        
        # å¦‚æœ caption_prompt ä¸ºç©ºä¸”é€‰æ‹©äº†é¢„è®¾ï¼Œåˆ™ä½¿ç”¨é¢„è®¾æç¤º
        if not caption_prompt.strip() and preset_selection != "æ— é¢„è®¾" and preset_selection in self.CAPTION_PRESETS:
            caption_prompt = self.CAPTION_PRESETS[preset_selection]
        
        print(f"[Qwen3-VL] è°ƒè¯•ä¿¡æ¯ - è¾“å…¥å‚æ•°:")
        print(f"  - æ¸…ç†ç¼“å­˜: {clear_cache}")
        print(f"  - å½“å‰ç¼“å­˜çŠ¶æ€: Model={self.model is not None}, Processor={self.processor is not None}")
        
        if clear_cache:
            self.cleanup_model()

        
        try:
            # ç‹¬ç«‹åŠ è½½æ¨¡å¼ - ä½¿ç”¨åŸå§‹åŠ è½½æ–¹æ³•
            print(f"[Qwen3-VL] ä½¿ç”¨ç‹¬ç«‹åŠ è½½æ¨¡å¼: {clean_model_name}")
            try:
                device_actual = self.get_device(device)
                self.load_model(clean_model_name, device_actual, quantization, attention_type)
                current_model = self.model
                current_processor = self.processor
                print(f"[Qwen3-VL Image Caption] æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as load_error:
                print(f"[Qwen3-VL Image Caption] æ¨¡å‹åŠ è½½å¤±è´¥: {str(load_error)}")
                raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(load_error)}ã€‚è¯·ç¡®ä¿ç½‘ç»œè¿æ¥æˆ–æ‰‹åŠ¨å°†æ¨¡å‹ä¸‹è½½åˆ° ComfyUI/models/LLM/Qwen-VL/ ç›®å½•")
            
            # è½¬æ¢å›¾åƒ
            pil_image = self.tensor_to_pil(image)
            
            # å‡†å¤‡æ¶ˆæ¯
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": pil_image},
                        {"type": "text", "text": caption_prompt}
                    ]
                }
            ]
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text = current_processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            
            # å¤„ç†è¾“å…¥
            inputs = current_processor(
                text=[text],
                images=[pil_image],
                return_tensors="pt",
                padding=True
            )
            
            # è·å–è®¾å¤‡ä¿¡æ¯
            device = next(current_model.parameters()).device
            
            # ç§»åŠ¨åˆ°é€‚å½“çš„è®¾å¤‡
            inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
            
            # ç”Ÿæˆæè¿°
            print(f"[Qwen3-VL Image Caption] å¼€å§‹ç”Ÿæˆå›¾åƒæè¿°...")
            
            # å¦‚æœæŒ‡å®šäº†ç§å­åˆ™è®¾ç½®ç§å­
            if seed != -1:
                torch.manual_seed(seed)
                if torch.cuda.is_available():
                    torch.cuda.manual_seed(seed)
                    torch.cuda.manual_seed_all(seed)
            
            with torch.no_grad():
                generated_ids = current_model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    temperature=temperature,
                    do_sample=True,
                    top_p=0.9,
                    pad_token_id=current_processor.tokenizer.pad_token_id,
                    eos_token_id=current_processor.tokenizer.eos_token_id
                )
            
            # è§£ç å“åº”
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs["input_ids"], generated_ids)
            ]
            
            response_text = current_processor.batch_decode(
                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=True
            )[0]
            
            print(f"[Qwen3-VL Image Caption] å›¾åƒæè¿°ç”Ÿæˆå®Œæˆ")
            
            # æå–çº¯æè¿°æ–‡æœ¬ï¼ˆç§»é™¤æ€è€ƒè¿‡ç¨‹æ ‡ç­¾ï¼‰
            import re
            clean_response = re.sub(r'<think>.*?</think>', '', response_text, flags=re.DOTALL).strip()
            
            return (clean_response, response_text)
            
        except Exception as e:
            error_msg = f"Image caption generation failed: {str(e)}"
            print(f"[Qwen3-VL Image Caption] {error_msg}")
            return (error_msg, f"Error details: {str(e)}")


# Node mappings
NODE_CLASS_MAPPINGS = {
    "Qwen3VLImageCaption": Qwen3VLImageCaption,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "Qwen3VLImageCaption": "Qwen3-VL å›¾åƒåæ¨",
}