import torch
import os
import json
import folder_paths
from PIL import Image
import numpy as np
from transformers import AutoModelForVision2Seq, AutoProcessor
from transformers import BitsAndBytesConfig
from huggingface_hub import snapshot_download
import gc


class Qwen3VLImageCaption:
    """Qwen3-VLå›¾ç‰‡åæ¨èŠ‚ç‚¹ - ä¸“é—¨ç”¨äºå›¾åƒæè¿°ç”Ÿæˆ"""
    
    def __init__(self):
        self.model = None
        self.processor = None
        self.current_model_name = None
        
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "å›¾åƒ": ("IMAGE",),
                "åæ¨æç¤ºè¯": ("STRING", {
                    "default": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ä¸»è¦ç‰©ä½“ã€åœºæ™¯ã€é¢œè‰²ã€é£æ ¼ç­‰ç‰¹å¾ã€‚",
                    "multiline": True
                }),
                "ç³»ç»Ÿæç¤ºè¯": ("STRING", {
                    "default": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å›¾åƒåˆ†æåŠ©æ‰‹ï¼Œèƒ½å¤Ÿå‡†ç¡®æè¿°å›¾ç‰‡ä¸­çš„å„ç§ç»†èŠ‚ã€‚",
                    "multiline": True
                }),
                "æœ€å¤§ä»¤ç‰Œæ•°": ("INT", {
                    "default": 512,
                    "min": 50,
                    "max": 2048,
                    "step": 50
                }),
                "æ¸©åº¦": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.1,
                    "max": 2.0,
                    "step": 0.1
                }),

            },
            "optional": {
                "æ¨¡å‹åç§°": (list(cls.MODEL_REPO_MAP.keys()), {
                    "default": "Qwen3-VL-2B-Instruct"
                }),
                "è®¾å¤‡": (["è‡ªåŠ¨", "cuda", "cpu", "mps"], {
                    "default": "è‡ªåŠ¨"
                }),
                "é‡åŒ–": (["æ— ", "4ä½é‡åŒ–", "8ä½é‡åŒ–"], {
                    "default": "4ä½é‡åŒ–"
                }),
                "æ³¨æ„åŠ›ç±»å‹": (["æ ‡å‡†æ³¨æ„åŠ›ï¼šå…¼å®¹æ€§å¥½", "SDPAæ³¨æ„åŠ›ï¼šå¹³è¡¡", "Flashæ³¨æ„åŠ›2ï¼šæ€§èƒ½ä¼˜å…ˆ"], {
                    "default": "æ ‡å‡†æ³¨æ„åŠ›ï¼šå…¼å®¹æ€§å¥½"
                }),
                "æ¸…é™¤ç¼“å­˜": ("BOOLEAN", {
                    "default": False
                }),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING")
    RETURN_NAMES = ("å›¾åƒæè¿°", "è¯¦ç»†å“åº”")
    FUNCTION = "generate_caption"
    CATEGORY = "QwenVL-ALL"
    
    MODEL_REPO_MAP = {
        "Qwen3-VL-2B-Instruct": "Qwen/Qwen3-VL-2B-Instruct",
        "Qwen3-VL-4B-Instruct": "Qwen/Qwen3-VL-4B-Instruct"
    }
    
    def get_device(self, device_preference):
        """è·å–è®¾å¤‡"""
        if device_preference == "è‡ªåŠ¨":
            if torch.cuda.is_available():
                return "cuda"
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                return "mps"
            else:
                return "cpu"
        return device_preference.lower()
    
    def _download_model_with_progress(self, model_id: str, local_dir: str):
        """ä¸‹è½½æ¨¡å‹å¹¶æ˜¾ç¤ºè¿›åº¦"""
        print(f"\n{'='*70}")
        print(f"[Qwen3-VLå›¾åƒåæ¨] ğŸ“¥ å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_id}")
        print(f"[Qwen3-VLå›¾åƒåæ¨] ğŸ“‚ ä¿å­˜è·¯å¾„: {local_dir}")
        print(f"[Qwen3-VLå›¾åƒåæ¨] â³ è¯·è€å¿ƒç­‰å¾…ï¼Œä¸‹è½½å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿåˆ°å‡ ååˆ†é’Ÿ...")
        print(f"[Qwen3-VLå›¾åƒåæ¨] ğŸ’¡ ä¸‹è½½è¿›åº¦å°†åœ¨ä¸‹æ–¹æ˜¾ç¤º")
        print(f"{'='*70}\n")

        try:
            # snapshot_download ä¼šè‡ªåŠ¨æ˜¾ç¤ºæ¯ä¸ªæ–‡ä»¶çš„ä¸‹è½½è¿›åº¦æ¡
            snapshot_download(
                repo_id=model_id,
                local_dir=local_dir,
                local_dir_use_symlinks=False,
                resume_download=True,
            )
            print(f"\n{'='*70}")
            print(f"[Qwen3-VLå›¾åƒåæ¨] âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {model_id}")
            print(f"{'='*70}\n")
        except Exception as e:
            print(f"\n{'='*70}")
            print(f"[Qwen3-VLå›¾åƒåæ¨] âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
            print(f"[Qwen3-VLå›¾åƒåæ¨] ğŸ’¡ è§£å†³æ–¹æ³•:")
            print(f"[Qwen3-VLå›¾åƒåæ¨]    1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print(f"[Qwen3-VLå›¾åƒåæ¨]    2. ä½¿ç”¨é•œåƒç«™: export HF_ENDPOINT=https://hf-mirror.com")
            print(f"[Qwen3-VLå›¾åƒåæ¨]    3. ä½¿ç”¨ä»£ç†: export HTTP_PROXY=http://127.0.0.1:7890")
            print(f"[Qwen3-VLå›¾åƒåæ¨]    4. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°: {local_dir}")
            print(f"{'='*70}\n")
            raise
    
    def load_model(self, model_name, device, quantization="æ— ", attention_type="æ ‡å‡†æ³¨æ„åŠ›ï¼šå…¼å®¹æ€§å¥½"):
        """åŠ è½½æ¨¡å‹"""
        if self.current_model_name == model_name and self.model is not None:
            return
            
        # æ¸…ç†ä¹‹å‰çš„æ¨¡å‹
        self.cleanup_model()
        
        # ä»æ˜ å°„ä¸­è·å–HuggingFaceä»“åº“ID
        repo_id = self.MODEL_REPO_MAP.get(model_name)
        if not repo_id:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")
        
        # ä½¿ç”¨æ¨¡å‹åç§°ä½œä¸ºæœ¬åœ°ç›®å½•åç§°
        model_checkpoint = os.path.join(
            folder_paths.models_dir, "LLM", "Qwen-VL", model_name
        )
        
        # å¦‚æœæ¨¡å‹ä¸å­˜åœ¨åˆ™ä¸‹è½½
        if not os.path.exists(model_checkpoint):
            self._download_model_with_progress(repo_id, model_checkpoint)
        
        try:
            print(f"[Qwen3-VLå›¾åƒåæ¨] åŠ è½½æ¨¡å‹: {model_name} åˆ°è®¾å¤‡: {device}")
            
            # è®¾ç½®æ¨¡å‹åŠ è½½å‚æ•°
            model_kwargs = {
                "dtype": torch.float16 if device == "cuda" else torch.float32,
                "device_map": "auto" if device == "cuda" else None,
                "low_cpu_mem_usage": True,
                "trust_remote_code": True
            }
            
            # é…ç½®é‡åŒ–
            quantization_config = None
            if quantization == "4ä½é‡åŒ–":
                quantization_config = BitsAndBytesConfig(load_in_4bit=True)
                model_kwargs["quantization_config"] = quantization_config
            elif quantization == "8ä½é‡åŒ–":
                quantization_config = BitsAndBytesConfig(load_in_8bit=True)
                model_kwargs["quantization_config"] = quantization_config
            
            # é…ç½®æ³¨æ„åŠ›ç±»å‹
            attention_type_map = {
                "æ ‡å‡†æ³¨æ„åŠ›ï¼šå…¼å®¹æ€§å¥½": "eager",
                "SDPAæ³¨æ„åŠ›ï¼šå¹³è¡¡": "sdpa", 
                "Flashæ³¨æ„åŠ›2ï¼šæ€§èƒ½ä¼˜å…ˆ": "flash_attention_2"
            }
            attn_implementation = attention_type_map.get(attention_type, "eager")
            model_kwargs["attn_implementation"] = attn_implementation
            
            # ä»æœ¬åœ°ç›®å½•åŠ è½½å¤„ç†å™¨å’Œæ¨¡å‹
            self.processor = AutoProcessor.from_pretrained(
                model_checkpoint,
                trust_remote_code=True
            )
            
            self.model = AutoModelForVision2Seq.from_pretrained(
                model_checkpoint,
                **model_kwargs
            )
            
            if device != "cuda":
                self.model = self.model.to(device)
                
            self.current_model_name = model_name
            print(f"[Qwen3-VLå›¾åƒåæ¨] æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
            
        except Exception as e:
            print(f"[Qwen3-VLå›¾åƒåæ¨] æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}ã€‚è¯·ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸ï¼Œæˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ° ComfyUI/models/LLM/Qwen-VL/ ç›®å½•")
    
    def cleanup_model(self):
        """æ¸…ç†æ¨¡å‹ç¼“å­˜"""
        if self.model is not None:
            del self.model
            self.model = None
        if self.processor is not None:
            del self.processor
            self.processor = None
        self.current_model_name = None
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def tensor_to_pil(self, tensor):
        """å°†Tensorè½¬æ¢ä¸ºPILå›¾åƒ"""
        if tensor.dim() == 4:
            tensor = tensor.squeeze(0)
        if tensor.dim() == 3 and tensor.shape[2] == 3:
            tensor = tensor.permute(2, 0, 1)
        
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        if tensor.max() <= 1.0:
            tensor = tensor * 255.0
        
        tensor = tensor.clamp(0, 255)
        array = tensor.cpu().numpy().astype(np.uint8)
        
        # ç¡®ä¿æ­£ç¡®çš„å½¢çŠ¶
        if array.shape[0] == 3:
            array = array.transpose(1, 2, 0)
        
        return Image.fromarray(array)
    
    def generate_caption(self, å›¾åƒ, åæ¨æç¤ºè¯, ç³»ç»Ÿæç¤ºè¯, æœ€å¤§ä»¤ç‰Œæ•°, æ¸©åº¦, æ¨¡å‹åç§°="Qwen3-VL-2B-Instruct", è®¾å¤‡="è‡ªåŠ¨", é‡åŒ–="4ä½é‡åŒ–", æ³¨æ„åŠ›ç±»å‹="æ ‡å‡†æ³¨æ„åŠ›ï¼šå…¼å®¹æ€§å¥½", æ¸…é™¤ç¼“å­˜=False):
        """ç”Ÿæˆå›¾åƒæè¿°"""
        
        print(f"[Qwen3-VL] è°ƒè¯•ä¿¡æ¯ - ä¼ å…¥å‚æ•°:")
        print(f"  - æ¸…é™¤ç¼“å­˜: {æ¸…é™¤ç¼“å­˜}")
        print(f"  - å½“å‰ç¼“å­˜çŠ¶æ€: æ¨¡å‹={self.model is not None}, å¤„ç†å™¨={self.processor is not None}")
        
        if æ¸…é™¤ç¼“å­˜:
            self.cleanup_model()
        

        
        try:
            # ç‹¬ç«‹åŠ è½½æ¨¡å¼ - ä½¿ç”¨åŸæ¥çš„åŠ è½½æ–¹å¼
            print(f"[Qwen3-VL] ä½¿ç”¨ç‹¬ç«‹åŠ è½½æ¨¡å¼: {æ¨¡å‹åç§°}")
            try:
                device_actual = self.get_device(è®¾å¤‡)
                self.load_model(æ¨¡å‹åç§°, device_actual, æ³¨æ„åŠ›ç±»å‹, é‡åŒ–)
                current_model = self.model
                current_processor = self.processor
                print(f"[Qwen3-VLå›¾åƒåæ¨] æ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as load_error:
                print(f"[Qwen3-VLå›¾åƒåæ¨] æ¨¡å‹åŠ è½½å¤±è´¥: {str(load_error)}")
                raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(load_error)}ã€‚è¯·ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸ï¼Œæˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ° ComfyUI/models/LLM/Qwen-VL/ ç›®å½•")
            
            # è½¬æ¢å›¾åƒ
            pil_image = self.tensor_to_pil(å›¾åƒ)
            
            # å‡†å¤‡æ¶ˆæ¯
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "image": pil_image},
                        {"type": "text", "text": åæ¨æç¤ºè¯}
                    ]
                }
            ]
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text = current_processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            
            # å¤„ç†è¾“å…¥
            inputs = current_processor(
                text=[text],
                images=[pil_image],
                return_tensors="pt",
                padding=True
            )
            
            # è·å–è®¾å¤‡ä¿¡æ¯
            device = next(current_model.parameters()).device
            
            # ç§»åŠ¨åˆ°ç›¸åº”è®¾å¤‡
            inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
            
            # ç”Ÿæˆæè¿°
            print(f"[Qwen3-VLå›¾åƒåæ¨] å¼€å§‹ç”Ÿæˆå›¾åƒæè¿°...")
            
            with torch.no_grad():
                generated_ids = current_model.generate(
                    **inputs,
                    max_new_tokens=æœ€å¤§ä»¤ç‰Œæ•°,
                    temperature=æ¸©åº¦,
                    do_sample=True,
                    top_p=0.9,
                    pad_token_id=current_processor.tokenizer.pad_token_id,
                    eos_token_id=current_processor.tokenizer.eos_token_id
                )
            
            # è§£ç å“åº”
            generated_ids_trimmed = [
                out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs["input_ids"], generated_ids)
            ]
            
            response_text = current_processor.batch_decode(
                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=True
            )[0]
            
            print(f"[Qwen3-VLå›¾åƒåæ¨] å›¾åƒæè¿°ç”Ÿæˆå®Œæˆ")
            
            # æå–çº¯æè¿°æ–‡æœ¬ï¼ˆç§»é™¤æ€è€ƒè¿‡ç¨‹æ ‡ç­¾ï¼‰
            import re
            clean_response = re.sub(r'<think>.*?</think>', '', response_text, flags=re.DOTALL).strip()
            
            return (clean_response, response_text)
            
        except Exception as e:
            error_msg = f"å›¾åƒæè¿°ç”Ÿæˆå¤±è´¥: {str(e)}"
            print(f"[Qwen3-VLå›¾åƒåæ¨] {error_msg}")
            return (error_msg, f"é”™è¯¯è¯¦æƒ…: {str(e)}")


# èŠ‚ç‚¹æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "Qwen3VLImageCaption": Qwen3VLImageCaption,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "Qwen3VLImageCaption": "Qwen3-VLå›¾åƒåæ¨",
}