# -*- coding: utf-8 -*-
"""
Qwen3-VL è§†é¢‘åæ¨èŠ‚ç‚¹
åŸºäºQwen3-VLæ¨¡å‹å®ç°è§†é¢‘å†…å®¹çš„æ™ºèƒ½æè¿°å’Œåˆ†æ
"""

import torch
import json
import numpy as np
from PIL import Image
from typing import Dict, List, Tuple, Any, Optional
import folder_paths
import comfy.model_management as model_management
import io
import base64
import locale
import sys

# è®¾ç½®é»˜è®¤ç¼–ç 
if sys.platform == 'win32':
    import _locale
    _locale._getdefaultlocale = (lambda *args: ['en_US', 'utf8'])
try:
    from transformers import Qwen3VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig
    QWEN3VL_AVAILABLE = True
except ImportError:
    print("[è­¦å‘Š] Qwen3VLForConditionalGeneration ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨å¤‡ç”¨æ¨¡å‹åŠ è½½æ–¹å¼")
    QWEN3VL_AVAILABLE = False
    from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig
from qwen_vl_utils import process_vision_info
import os
from pathlib import Path
try:
    from huggingface_hub import snapshot_download
    HF_HUB_AVAILABLE = True
except ImportError:
    HF_HUB_AVAILABLE = False


class Qwen3VLVideoCaption:
    """
    Qwen3-VL è§†é¢‘åæ¨èŠ‚ç‚¹
    æ”¯æŒè§†é¢‘å…³é”®å¸§æå–ã€å†…å®¹æè¿°ã€åŠ¨ä½œåˆ†æç­‰åŠŸèƒ½
    """
    
    # æ¨¡å‹IDæ˜ å°„ï¼šæ¨¡å‹åç§° -> HuggingFaceä»“åº“ID
    MODEL_REPO_MAP = {
        "Qwen3-VL-2B-Instruct": "Qwen/Qwen3-VL-2B-Instruct",
        "Qwen3-VL-4B-Instruct": "Qwen/Qwen3-VL-4B-Instruct"
    }
    
    def __init__(self):
        # åˆå§‹åŒ–æ¨¡å‹ç¼“å­˜
        self.model_cache = {}
        self.processor_cache = {}
        self.current_model_name = None
    
    @classmethod
    def INPUT_TYPES(cls) -> Dict[str, Any]:
        return {
            "required": {
                "video_frames": ("IMAGE", {
                    "description": "è¾“å…¥è§†é¢‘å¸§åºåˆ—"
                }),
                "caption_mode": (["è¯¦ç»†æè¿°", "ç®€æ´æè¿°", "åŠ¨ä½œåˆ†æ", "åœºæ™¯åˆ†æ", "æƒ…æ„Ÿåˆ†æ", "è‡ªå®šä¹‰"], {
                    "default": "è¯¦ç»†æè¿°",
                    "description": "é€‰æ‹©è§†é¢‘æè¿°æ¨¡å¼"
                }),
                "max_tokens": ("INT", {
                    "default": 512,
                    "min": 50,
                    "max": 2048,
                    "step": 50,
                    "description": "æœ€å¤§ä»¤ç‰Œæ•°"
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.1,
                    "max": 2.0,
                    "step": 0.1,
                    "description": "æ¸©åº¦å‚æ•°"
                })
            },
            "optional": {
                "model_name": (["Qwen3-VL-2B-Instruct", "Qwen3-VL-4B-Instruct"], 
                                 {"default": "Qwen3-VL-2B-Instruct",
                                  "description": "é€‰æ‹©Qwen3-VLæ¨¡å‹ç‰ˆæœ¬"}),
                "device": (["auto", "cpu", "cuda", "mps"], {
                    "default": "auto",
                    "description": "è®¾å¤‡é€‰æ‹©"
                }),
                "attention_type": (["æ ‡å‡†æ³¨æ„åŠ›", "SDPAæ³¨æ„åŠ›", "Flashæ³¨æ„åŠ›2"], {
                    "default": "SDPAæ³¨æ„åŠ›",
                    "description": "æ³¨æ„åŠ›ç±»å‹"
                }),
                "quantization": (["æ— ", "4ä½é‡åŒ–", "8ä½é‡åŒ–"], {
                    "default": "4ä½é‡åŒ–",
                    "description": "é‡åŒ–æ¨¡å¼"
                }),
                "custom_prompt": ("STRING", {
                    "default": "è¯·è¯¦ç»†æè¿°è¿™ä¸ªè§†é¢‘çš„å†…å®¹ï¼ŒåŒ…æ‹¬åœºæ™¯ã€åŠ¨ä½œã€æƒ…æ„Ÿç­‰",
                    "multiline": True,
                    "description": "è‡ªå®šä¹‰æç¤ºè¯(ä»…åœ¨è‡ªå®šä¹‰æ¨¡å¼ä¸‹ä½¿ç”¨)"
                }),
                "frame_sampling": (["å‡åŒ€é‡‡æ ·", "å…³é”®å¸§æå–", "å…¨éƒ¨å¸§"], {
                    "default": "å‡åŒ€é‡‡æ ·",
                    "description": "å¸§é‡‡æ ·ç­–ç•¥"
                }),
                "sample_rate": ("INT", {
                    "default": 8,
                    "min": 1,
                    "max": 60,
                    "step": 1,
                    "description": "é‡‡æ ·å¸§æ•°"
                }),
                "output_format": (["çº¯æ–‡æœ¬", "JSONæ ¼å¼", "Markdownæ ¼å¼"], {
                    "default": "çº¯æ–‡æœ¬",
                    "description": "è¾“å‡ºæ ¼å¼"
                }),
                "keep_model_loaded": ("BOOLEAN", {
                    "default": False,
                    "description": "ä¿æŒæ¨¡å‹åŠ è½½çŠ¶æ€(å‡å°‘é‡å¤åŠ è½½æ—¶é—´ä½†å ç”¨æ›´å¤šå†…å­˜)"
                })
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING", "DICT")
    RETURN_NAMES = ("è§†é¢‘æè¿°", "è¯¦ç»†å“åº”", "å¤„ç†ä¿¡æ¯")
    OUTPUT_NODE = True
    FUNCTION = "generate_video_caption"
    CATEGORY = "QwenVL-ALL"
    
    def _get_device_info(self, device: str) -> Dict[str, Any]:
        """è·å–è®¾å¤‡ä¿¡æ¯"""
        if device == "auto":
            if torch.cuda.is_available():
                device = "cuda"
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                device = "mps"
            else:
                device = "cpu"
        
        device_info = {
            "device": device,
            "device_name": torch.cuda.get_device_name() if device == "cuda" else device,
            "memory_free": torch.cuda.mem_get_info()[0] if device == "cuda" else 0,
            "memory_total": torch.cuda.mem_get_info()[1] if device == "cuda" else 0
        }
        
        return device_info
    
    def _download_model_with_progress(self, model_id: str, local_dir: str):
        """ä¸‹è½½æ¨¡å‹å¹¶æ˜¾ç¤ºè¿›åº¦"""
        print(f"\n{'='*70}")
        print(f"[Qwen3-VL è§†é¢‘åæ¨] ğŸ“¥ å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_id}")
        print(f"[Qwen3-VL è§†é¢‘åæ¨] ğŸ“‚ ä¿å­˜è·¯å¾„: {local_dir}")
        print(f"[Qwen3-VL è§†é¢‘åæ¨] â³ è¯·è€å¿ƒç­‰å¾…ï¼Œä¸‹è½½å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿåˆ°å‡ ååˆ†é’Ÿ...")
        print(f"[Qwen3-VL è§†é¢‘åæ¨] ğŸ’¡ ä¸‹è½½è¿›åº¦å°†åœ¨ä¸‹æ–¹æ˜¾ç¤º")
        print(f"{'='*70}\n")

        try:
            # snapshot_download ä¼šè‡ªåŠ¨æ˜¾ç¤ºæ¯ä¸ªæ–‡ä»¶çš„ä¸‹è½½è¿›åº¦æ¡
            snapshot_download(
                repo_id=model_id,
                local_dir=local_dir,
                local_dir_use_symlinks=False,
                resume_download=True,
            )
            print(f"\n{'='*70}")
            print(f"[Qwen3-VL è§†é¢‘åæ¨] âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {model_id}")
            print(f"{'='*70}\n")
        except Exception as e:
            print(f"\n{'='*70}")
            print(f"[Qwen3-VL è§†é¢‘åæ¨] âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
            print(f"[Qwen3-VL è§†é¢‘åæ¨] ğŸ’¡ è§£å†³æ–¹æ³•:")
            print(f"[Qwen3-VL è§†é¢‘åæ¨]    1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print(f"[Qwen3-VL è§†é¢‘åæ¨]    2. ä½¿ç”¨é•œåƒç«™: export HF_ENDPOINT=https://hf-mirror.com")
            print(f"[Qwen3-VL è§†é¢‘åæ¨]    3. ä½¿ç”¨ä»£ç†: export HTTP_PROXY=http://127.0.0.1:7890")
            print(f"[Qwen3-VL è§†é¢‘åæ¨]    4. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°: {local_dir}")
            print(f"{'='*70}\n")
            raise
    
    def _process_attention_type(self, attention_type: str) -> str:
        """å¤„ç†æ³¨æ„åŠ›ç±»å‹"""
        attention_map = {
            "æ ‡å‡†æ³¨æ„åŠ›": "eager",
            "SDPAæ³¨æ„åŠ›": "sdpa", 
            "Flashæ³¨æ„åŠ›2": "flash_attention_2"
        }
        
        for key, value in attention_map.items():
            if key in attention_type:
                return value
        
        return "sdpa"  # é»˜è®¤è¿”å›SDPA
    
    def _load_model(self, model_name: str, device: str, quantization: str, attn_implementation: str):
        """åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨ï¼Œä½¿ç”¨æŒ‡å®šé…ç½®"""
        # æ£€æŸ¥ç¼“å­˜
        if self.current_model_name == model_name and self.model_cache.get(model_name) is not None:
            return self.model_cache[model_name], self.processor_cache.get(model_name)
        
        # ä»æ˜ å°„ä¸­è·å–HuggingFaceä»“åº“IDï¼Œå›é€€åˆ°Qwen/{model_name}
        model_id = self.MODEL_REPO_MAP.get(model_name, f"Qwen/{model_name}")
        
        # ä½¿ç”¨æ¨¡å‹åç§°ä½œä¸ºæœ¬åœ°ç›®å½•åç§°
        model_checkpoint = os.path.join(
            folder_paths.models_dir, "LLM", "Qwen-VL", model_name
        )
        
        # å¦‚æœæ¨¡å‹ä¸å­˜åœ¨åˆ™ä¸‹è½½
        if not os.path.exists(model_checkpoint) and HF_HUB_AVAILABLE:
            self._download_model_with_progress(model_id, model_checkpoint)
        elif not os.path.exists(model_checkpoint):
            print(f"[è­¦å‘Š] æ¨¡å‹ç›®å½•ä¸å­˜åœ¨ä¸”huggingface_hubä¸å¯ç”¨ï¼Œå°†å°è¯•ç›´æ¥åŠ è½½: {model_name}")
            model_checkpoint = model_name  # å›é€€åˆ°ç›´æ¥åŠ è½½
        
        try:
            # åŠ è½½å¤„ç†å™¨
            processor = AutoProcessor.from_pretrained(
                model_checkpoint,
                trust_remote_code=True,
                local_files_only=False  # å…è®¸ä¸‹è½½å¤„ç†å™¨æ–‡ä»¶
            )
            
            # é…ç½®é‡åŒ–
            quantization_config = None
            if quantization == "4ä½é‡åŒ–":
                quantization_config = BitsAndBytesConfig(load_in_4bit=True)
            elif quantization == "8ä½é‡åŒ–":
                quantization_config = BitsAndBytesConfig(load_in_8bit=True)
            
            # æ£€æµ‹bf16æ”¯æŒ
            bf16_support = (
                torch.cuda.is_available()
                and torch.cuda.get_device_capability(device if device != "auto" else torch.device("cuda"))[0] >= 8
            ) if device != "cpu" else False
            
            # é€‰æ‹©æ¨¡å‹åŠ è½½ç±»
            if QWEN3VL_AVAILABLE:
                model_class = Qwen3VLForConditionalGeneration
            else:
                print("[è­¦å‘Š] Qwen3VLForConditionalGenerationä¸å¯ç”¨ï¼Œä½¿ç”¨AutoModelForVision2Seq")
                model_class = AutoModelForVision2Seq
            
            # åŠ è½½æ¨¡å‹
            model = model_class.from_pretrained(
                model_checkpoint,
                dtype=torch.bfloat16 if bf16_support else torch.float16,
                device_map="auto" if device == "auto" else None,
                attn_implementation=attn_implementation,
                quantization_config=quantization_config,
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                local_files_only=False  # å…è®¸ä¸‹è½½æ¨¡å‹æ–‡ä»¶
            )
            
            # è®¾å¤‡ç‰¹å®šå¤„ç†
            if device != "auto" and device != "cpu" and quantization == "æ— ":
                model = model.to(device)
            
            # ç¼“å­˜æ¨¡å‹
            self.model_cache[model_name] = model
            self.processor_cache[model_name] = processor
            self.current_model_name = model_name
            
            return model, processor
            
        except Exception as e:
            error_msg = f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
            print(f"[Qwen3-VL Video Caption] {error_msg}")
            import traceback
            traceback.print_exc()
            raise RuntimeError(error_msg)
        
        # ç¡®ä¿æ¨¡å‹æ–‡ä»¶ç¼–ç æ­£ç¡®
        import locale
        encoding = locale.getpreferredencoding()
        print(f"[Qwen3-VL Video Caption] ç³»ç»Ÿç¼–ç : {encoding}")
    
    def generate_video_caption(self, video_frames, caption_mode, max_tokens, 
                             temperature, model_name="Qwen3-VL-2B-Instruct", device="auto", 
                             attention_type="SDPAæ³¨æ„åŠ›", quantization="4ä½é‡åŒ–",
                             custom_prompt=None, frame_sampling="å‡åŒ€é‡‡æ ·", sample_rate=8,
                             output_format="çº¯æ–‡æœ¬", keep_model_loaded=False) -> Tuple[str, str, Dict[str, Any]]:
        """
        ç”Ÿæˆè§†é¢‘æè¿°
        
        Args:
            video_frames: è¾“å…¥è§†é¢‘å¸§åºåˆ—
            model_name: æ¨¡å‹åç§°
            caption_mode: æè¿°æ¨¡å¼
            max_tokens: æœ€å¤§ä»¤ç‰Œæ•°
            temperature: æ¸©åº¦å‚æ•°
            device: è®¾å¤‡é€‰æ‹©
            attention_type: æ³¨æ„åŠ›ç±»å‹
            quantization: é‡åŒ–æ¨¡å¼
            custom_prompt: è‡ªå®šä¹‰æç¤ºè¯
            frame_sampling: å¸§é‡‡æ ·ç­–ç•¥
            sample_rate: é‡‡æ ·å¸§æ•°
            output_format: è¾“å‡ºæ ¼å¼
            keep_model_loaded: æ˜¯å¦ä¿æŒæ¨¡å‹åŠ è½½çŠ¶æ€
            
        Returns:
            Tuple[str, str, Dict]: è§†é¢‘æè¿°ã€è¯¦ç»†å“åº”ã€å¤„ç†ä¿¡æ¯
        """
        try:
            # å¸§é‡‡æ ·å¤„ç†
            sampled_frames = self._sample_frames(video_frames, frame_sampling, sample_rate)
            
            # æ„å»ºæç¤ºè¯
            prompt = self._build_prompt(caption_mode, custom_prompt)
            
            # å¤„ç†è®¾å¤‡ä¿¡æ¯
            device_info = self._get_device_info(device)
            
            # å¤„ç†æ³¨æ„åŠ›ç±»å‹
            attn_implementation = self._process_attention_type(attention_type)
            
            # ç‹¬ç«‹åŠ è½½æ¨¡å‹
            print(f"[Qwen3-VL è§†é¢‘åæ¨] ç‹¬ç«‹åŠ è½½æ¨¡å‹: {model_name}")
            model, processor = self._load_model(
                model_name=model_name,
                device=device_info["device"],
                quantization=quantization,
                attn_implementation=attn_implementation
            )
            
            # å¤„ç†è§†é¢‘å¸§ä¸ºbase64ç¼–ç 
            image_data = []
            if sampled_frames.dim() == 4:  # (T, H, W, C)
                frames = (sampled_frames * 255).byte().cpu().numpy()
                for frame in frames:
                    # è½¬æ¢å¼ é‡åˆ°PILå›¾åƒ
                    pil_image = Image.fromarray(frame)
                    
                    # å°†PILå›¾åƒè½¬æ¢ä¸ºbase64
                    img_buffer = io.BytesIO()
                    pil_image.save(img_buffer, format='PNG')
                    img_buffer.seek(0)
                    img_b64 = base64.b64encode(img_buffer.getvalue()).decode('utf-8')
                    image_data.append(f"data:image/png;base64,{img_b64}")
            
            # æ„å»ºæ¶ˆæ¯
            messages = self._prepare_messages(prompt, image_data, None)
            
            with torch.no_grad():
                # åº”ç”¨èŠå¤©æ¨¡æ¿
                text = processor.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                
                # å¤„ç†è§†è§‰ä¿¡æ¯
                image_inputs, video_inputs, video_kwargs = process_vision_info(
                    messages,
                    return_video_kwargs=True
                )
                
                # å¦‚æœfpsæ˜¯åºåˆ—åˆ™ä¿®å¤video_kwargs
                if video_kwargs and 'fps' in video_kwargs:
                    fps_value = video_kwargs['fps']
                    # å¦‚æœfpsæ˜¯åºåˆ—ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
                    if isinstance(fps_value, (list, tuple)):
                        video_kwargs['fps'] = fps_value[0] if fps_value else 24
                
                # å‡†å¤‡è¾“å…¥
                inputs = processor(
                    text=[text],
                    images=image_inputs,
                    videos=video_inputs,
                    padding=True,
                    return_tensors="pt",
                    **video_kwargs
                )
                
                # è·å–è®¾å¤‡ä¿¡æ¯
                if hasattr(model, 'device'):
                    device = model.device
                else:
                    device = next(model.parameters()).device
                
                inputs = inputs.to(device)
                
                # ç”Ÿæˆ
                generated_ids = model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    temperature=temperature,
                    do_sample=True,
                    pad_token_id=processor.tokenizer.eos_token_id,
                )
                
                generated_ids_trimmed = [
                    out_ids[len(in_ids):]
                    for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
                ]
                
                result = processor.batch_decode(
                    generated_ids_trimmed,
                    skip_special_tokens=True,
                    clean_up_tokenization_spaces=False,
                )
                
                output_text = result[0] if result else ""
            
            # æ¸…ç†æ¨¡å‹ç¼“å­˜ï¼ˆä»…åœ¨ä¸éœ€è¦ä¿æŒåŠ è½½æ—¶ï¼‰
            if not keep_model_loaded:
                if model_name in self.model_cache:
                    del self.model_cache[model_name]
                if model_name in self.processor_cache:
                    del self.processor_cache[model_name]
                self.current_model_name = None
                
                # æ¸…ç†GPUå†…å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.ipc_collect()
            
            # æ ¼å¼åŒ–è¾“å‡º
            formatted_output = self._format_output(output_text, output_format)
            
            # æ„å»ºå¤„ç†ä¿¡æ¯
            process_info = {
                "model_name": model_name,
                "caption_mode": caption_mode,
                "frame_count": len(sampled_frames),
                "total_frames": len(video_frames),
                "device": str(device),
                "attention_type": attention_type,
                "quantization": quantization,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "processing_time": "N/A",  # å¯ä»¥æ·»åŠ å®é™…å¤„ç†æ—¶é—´
                "output_length": len(output_text),
                "frame_sampling": frame_sampling,
                "sample_rate": sample_rate,
                "keep_model_loaded": keep_model_loaded
            }
            
            return (formatted_output, output_text, process_info)
            
        except Exception as e:
            error_msg = f"è§†é¢‘æè¿°ç”Ÿæˆå¤±è´¥: {str(e)}"
            error_info = {
                "error": error_msg,
                "model_name": model_name,
                "caption_mode": caption_mode,
                "device": device,
                "frame_sampling": frame_sampling,
                "keep_model_loaded": keep_model_loaded
            }
            return (error_msg, error_msg, error_info)
    
    def _sample_frames(self, video_frames: torch.Tensor, strategy: str, rate: int) -> torch.Tensor:
        """å¸§é‡‡æ ·å¤„ç†"""
        total_frames = len(video_frames)
        
        if strategy == "å…¨éƒ¨å¸§":
            return video_frames
        elif strategy == "å…³é”®å¸§æå–":
            # ç®€å•çš„å…³é”®å¸§æå–é€»è¾‘ï¼ˆå¯ä»¥æ”¹è¿›ä¸ºæ›´å¤æ‚çš„ç®—æ³•ï¼‰
            indices = torch.linspace(0, total_frames-1, min(rate, total_frames), dtype=torch.long)
            return video_frames[indices]
        else:  # å‡åŒ€é‡‡æ ·
            indices = torch.linspace(0, total_frames-1, min(rate, total_frames), dtype=torch.long)
            return video_frames[indices]
    
    def _build_prompt(self, mode: str, custom_prompt: Optional[str] = None) -> str:
        """æ„å»ºæç¤ºè¯"""
        prompts = {
            "è¯¦ç»†æè¿°": "è¯·è¯¦ç»†æè¿°è¿™ä¸ªè§†é¢‘çš„å†…å®¹ï¼ŒåŒ…æ‹¬åœºæ™¯ã€äººç‰©ã€åŠ¨ä½œã€æƒ…æ„Ÿã€æ°›å›´ç­‰æ–¹é¢ã€‚",
            "ç®€æ´æè¿°": "è¯·ç®€æ´åœ°æè¿°è¿™ä¸ªè§†é¢‘çš„ä¸»è¦å†…å®¹ã€‚",
            "åŠ¨ä½œåˆ†æ": "è¯·åˆ†æè§†é¢‘ä¸­çš„åŠ¨ä½œå’Œè¡Œä¸ºï¼Œæè¿°å‘ç”Ÿäº†ä»€ä¹ˆã€‚",
            "åœºæ™¯åˆ†æ": "è¯·æè¿°è§†é¢‘ä¸­çš„åœºæ™¯ã€ç¯å¢ƒå’ŒèƒŒæ™¯ã€‚",
            "æƒ…æ„Ÿåˆ†æ": "è¯·åˆ†æè§†é¢‘ä¸­è¡¨è¾¾çš„æƒ…æ„Ÿå’Œæ°›å›´ã€‚"
        }
        
        if mode == "è‡ªå®šä¹‰" and custom_prompt:
            return custom_prompt
        else:
            return prompts.get(mode, prompts["è¯¦ç»†æè¿°"])
    
    def _process_video_frames(self, frames: torch.Tensor, processor) -> List[Image.Image]:
        """å¤„ç†è§†é¢‘å¸§"""
        processed_images = []
        
        for frame in frames:
            # è½¬æ¢å¼ é‡åˆ°PILå›¾åƒ
            if isinstance(frame, torch.Tensor):
                # å‡è®¾å¸§æ˜¯ [H, W, C] æ ¼å¼ï¼Œå€¼èŒƒå›´ [0, 1]
                frame_np = (frame.cpu().numpy() * 255).astype(np.uint8)
                image = Image.fromarray(frame_np)
            else:
                image = frame
            
            processed_images.append(image)
        
        return processed_images
    
    def _build_messages(self, prompt: str, images: List[Image.Image]) -> List[Dict[str, Any]]:
        """æ„å»ºå¯¹è¯æ¶ˆæ¯"""
        content = [{"type": "text", "text": prompt}]
        
        # æ·»åŠ å›¾åƒ
        for image in images:
            content.append({"type": "image", "image": image})
        
        messages = [
            {
                "role": "user",
                "content": content
            }
        ]
        
        return messages
    
    def _prepare_messages(self, text_prompt: str, image_data: Optional[List[str]], video_data: Optional[str]) -> List[Dict[str, Any]]:
        """å‡†å¤‡æ¶ˆæ¯å†…å®¹ï¼Œæ”¯æŒæ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘"""
        messages = []
        content = [{"type": "text", "text": text_prompt}]
        
        # æ·»åŠ å›¾åƒæ•°æ®
        if image_data:
            for img_data in image_data:
                content.append({"type": "image", "image": img_data})
        
        # æ·»åŠ è§†é¢‘æ•°æ®
        if video_data:
            content.append({"type": "video", "video": video_data})
        
        messages.append({"role": "user", "content": content})
        return messages
    
    def _format_output(self, text: str, format_type: str) -> str:
        """æ ¼å¼åŒ–è¾“å‡º"""
        if format_type == "JSONæ ¼å¼":
            try:
                # å°è¯•å°†æ–‡æœ¬è§£æä¸ºJSONæ ¼å¼
                structured = {
                    "description": text,
                    "summary": text[:200] + "..." if len(text) > 200 else text,
                    "length": len(text)
                }
                return json.dumps(structured, ensure_ascii=False, indent=2)
            except:
                return text
        elif format_type == "Markdownæ ¼å¼":
            return f"## è§†é¢‘æè¿°\n\n{text}\n\n---\n*ç”Ÿæˆæ—¶é—´: {torch.cuda.Event().elapsed_time()}ms*"
        else:
            return text


# èŠ‚ç‚¹æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "Qwen3VLVideoCaption": Qwen3VLVideoCaption
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "Qwen3VLVideoCaption": "Qwen3-VL è§†é¢‘åæ¨"
}