# -*- coding: utf-8 -*-
"""
ComfyUIçš„Qwen3-VLè§†é¢‘æè¿°èŠ‚ç‚¹
æä¾›ä½¿ç”¨Qwen3-VLæ¨¡å‹çš„è§†é¢‘æè¿°åŠŸèƒ½
"""

import torch
import json
import numpy as np
from PIL import Image
from typing import Dict, List, Tuple, Any, Optional, Union
import folder_paths
import comfy.model_management as model_management
import io
import base64
import locale
import sys

# è®¾ç½®é»˜è®¤ç¼–ç 
if sys.platform == 'win32':
    import _locale
    _locale._getdefaultlocale = (lambda *args: ['en_US', 'utf8'])
try:
    from transformers import Qwen3VLForConditionalGeneration, AutoProcessor, BitsAndBytesConfig
    QWEN3VL_AVAILABLE = True
except ImportError:
    print("[Warning] Qwen3VLForConditionalGeneration not available, will use fallback model loading method")
    QWEN3VL_AVAILABLE = False
    from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig
from qwen_vl_utils import process_vision_info
import os
from pathlib import Path
try:
    from huggingface_hub import snapshot_download
    HF_HUB_AVAILABLE = True
except ImportError:
    HF_HUB_AVAILABLE = False


class Qwen3VLVideoCaption:
    """
    Qwen3-VLè§†é¢‘æè¿°èŠ‚ç‚¹
    æä¾›æ™ºèƒ½è§†é¢‘å†…å®¹æè¿°å’Œåˆ†æåŠŸèƒ½
    """
    
    # åŸºç¡€æ¨¡å‹æ˜ å°„
    BASE_MODEL_REPO_MAP = {
        "Qwen3-VL-2B-Instruct": "Qwen/Qwen3-VL-2B-Instruct",
        "Qwen3-VL-2B-Thinking": "Qwen/Qwen3-VL-2B-Thinking",
        "Qwen3-VL-2B-Instruct-FP8": "Qwen/Qwen3-VL-2B-Instruct-FP8",
        "Qwen3-VL-2B-Thinking-FP8": "Qwen/Qwen3-VL-2B-Thinking-FP8",
        "Qwen3-VL-4B-Instruct": "Qwen/Qwen3-VL-4B-Instruct",
        "Qwen3-VL-4B-Thinking": "Qwen/Qwen3-VL-4B-Thinking",
        "Qwen3-VL-4B-Instruct-FP8": "Qwen/Qwen3-VL-4B-Instruct-FP8",
        "Qwen3-VL-4B-Thinking-FP8": "Qwen/Qwen3-VL-4B-Thinking-FP8",
        "Qwen3-VL-8B-Instruct": "Qwen/Qwen3-VL-8B-Instruct",
        "Qwen3-VL-8B-Thinking": "Qwen/Qwen3-VL-8B-Thinking",
        "Qwen3-VL-8B-Instruct-FP8": "Qwen/Qwen3-VL-8B-Instruct-FP8",
        "Qwen3-VL-8B-Thinking-FP8": "Qwen/Qwen3-VL-8B-Thinking-FP8"
    }
    
    @classmethod
    def get_available_models(cls):
        """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨ï¼ŒåŒ…æ‹¬åŸºç¡€æ¨¡å‹å’Œæœ¬åœ°æ¨¡å‹æ–‡ä»¶å¤¹ä¸­çš„æ¨¡å‹"""
        # è·å–åŸºç¡€æ¨¡å‹åˆ—è¡¨ï¼Œå¹¶æ£€æŸ¥æœ¬åœ°æ˜¯å¦å­˜åœ¨
        available_models = []
        models_dir = os.path.join(folder_paths.models_dir, "LLM", "Qwen-VL")
        
        # æ£€æŸ¥åŸºç¡€æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½
        for model_name in cls.BASE_MODEL_REPO_MAP.keys():
            model_path = os.path.join(models_dir, model_name)
            if os.path.exists(model_path):
                available_models.append(f"{model_name}ï¼ˆå·²ä¸‹è½½ï¼‰")
            else:
                available_models.append(model_name)
        
        # æ‰«ææœ¬åœ°æ¨¡å‹æ–‡ä»¶å¤¹ï¼Œæ·»åŠ æœªåœ¨åŸºç¡€æ¨¡å‹åˆ—è¡¨ä¸­çš„æ¨¡å‹
        if os.path.exists(models_dir):
            for item in os.listdir(models_dir):
                item_path = os.path.join(models_dir, item)
                # æ¸…ç†æ¨¡å‹åç§°ï¼ˆç§»é™¤å¯èƒ½çš„"ï¼ˆå·²ä¸‹è½½ï¼‰"æ ‡è®°ï¼‰
                clean_item = item.replace("ï¼ˆå·²ä¸‹è½½ï¼‰", "")
                # æ£€æŸ¥æ˜¯å¦ä¸ºç›®å½•ä¸”ä¸åœ¨åŸºç¡€æ¨¡å‹åˆ—è¡¨ä¸­
                if os.path.isdir(item_path) and clean_item not in cls.BASE_MODEL_REPO_MAP.keys():
                    # æ£€æŸ¥æ˜¯å¦å·²æ·»åŠ è¿‡ï¼ˆé¿å…é‡å¤ï¼‰
                    if not any(clean_item == m.replace("ï¼ˆå·²ä¸‹è½½ï¼‰", "") for m in available_models):
                        available_models.append(f"{item}ï¼ˆå·²ä¸‹è½½ï¼‰")
        
        return available_models

    # åŠ¨æ€æ¨¡å‹æ˜ å°„ï¼Œç»“åˆåŸºç¡€æ¨¡å‹å’Œæœ¬åœ°æ¨¡å‹
    @classmethod
    def get_model_repo_map(cls):
        """è·å–æ¨¡å‹ä»“åº“æ˜ å°„ï¼ŒåŒ…æ‹¬åŸºç¡€æ¨¡å‹å’Œæœ¬åœ°æ¨¡å‹"""
        model_repo_map = cls.BASE_MODEL_REPO_MAP.copy()
        
        # æ‰«ææœ¬åœ°æ¨¡å‹æ–‡ä»¶å¤¹
        models_dir = os.path.join(folder_paths.models_dir, "LLM", "Qwen-VL")
        if os.path.exists(models_dir):
            for item in os.listdir(models_dir):
                item_path = os.path.join(models_dir, item)
                # æå–å¹²å‡€çš„æ¨¡å‹åç§°ï¼ˆå»é™¤"ï¼ˆå·²ä¸‹è½½ï¼‰"æ ‡è®°ï¼‰
                clean_item = item.replace("ï¼ˆå·²ä¸‹è½½ï¼‰", "")
                # æ£€æŸ¥æ˜¯å¦ä¸ºç›®å½•ä¸”ä¸åœ¨æ˜ å°„ä¸­
                if os.path.isdir(item_path) and clean_item not in model_repo_map:
                    # å¯¹äºæœ¬åœ°æ¨¡å‹ï¼Œä½¿ç”¨ç›®å½•åä½œä¸ºä»“åº“ID
                    model_repo_map[clean_item] = clean_item
        
        return model_repo_map
    
    def __init__(self):
        # åˆå§‹åŒ–æ¨¡å‹ç¼“å­˜
        self.model_cache = {}
        self.processor_cache = {}
        self.current_model_name = None
    
    @classmethod
    def INPUT_TYPES(cls) -> Dict[str, Any]:
        return {
            "required": {
                "video_frames": ("IMAGE", {
                    "description": "è¾“å…¥è§†é¢‘å¸§åºåˆ—"
                }),
                "caption_mode": (["è¯¦ç»†æè¿°", "ç®€æ´æè¿°", "åŠ¨ä½œåˆ†æ", "åœºæ™¯åˆ†æ", "æƒ…æ„Ÿåˆ†æ", "è‡ªå®šä¹‰"], {
                    "default": "è¯¦ç»†æè¿°",
                    "description": "é€‰æ‹©æè¿°ç”Ÿæˆæ¨¡å¼"
                }),
                "max_tokens": ("INT", {
                    "default": 512,
                    "min": 50,
                    "max": 2048,
                    "step": 50,
                    "description": "æœ€å¤§ç”Ÿæˆä»¤ç‰Œæ•°"
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.1,
                    "max": 2.0,
                    "step": 0.1,
                    "description": "ç”Ÿæˆæ¸©åº¦å‚æ•°"
                }),
                "repetition_penalty": ("FLOAT", {
                    "default": 1.2,
                    "min": 0.1,
                    "max": 2.0,
                    "step": 0.1,
                    "description": "é‡å¤æƒ©ç½šå‚æ•°ï¼Œæ§åˆ¶é‡å¤å†…å®¹çš„ç”Ÿæˆ"
                }),

            },
            "optional": {
                "model_name": (cls.get_available_models(), 
                                 {"default": "Qwen3-VL-2B-Instruct",
                                  "description": "é€‰æ‹©Qwen3-VLæ¨¡å‹ç‰ˆæœ¬"}),
                "device": (["auto", "cpu", "cuda", "mps"], {
                    "default": "auto",
                    "description": "è®¾å¤‡é€‰æ‹©"
                }),
                "attention_type": (["Eageræ³¨æ„åŠ›", "SDPAæ³¨æ„åŠ›", "Flashæ³¨æ„åŠ›2"], {
                    "default": "SDPAæ³¨æ„åŠ›",
                    "description": "æ³¨æ„åŠ›ç±»å‹"
                }),
                "quantization": (["æ— ï¼ˆFP16ï¼‰", "4ä½", "8ä½"], {
                    "default": "æ— ï¼ˆFP16ï¼‰",
                    "description": "é‡åŒ–æ¨¡å¼"
                }),
                "custom_prompt": ("STRING", {
                    "default": "è¯·è¯¦ç»†æè¿°è¿™ä¸ªè§†é¢‘çš„å†…å®¹ï¼ŒåŒ…æ‹¬åœºæ™¯ã€åŠ¨ä½œã€æƒ…æ„Ÿç­‰ã€‚",
                    "multiline": True,
                    "description": "è‡ªå®šä¹‰æç¤ºè¯ï¼ˆä»…åœ¨è‡ªå®šä¹‰æ¨¡å¼ä¸‹ä½¿ç”¨ï¼‰"
                }),
                "frame_sampling": (["å‡åŒ€é‡‡æ ·", "å…³é”®å¸§æå–", "æ‰€æœ‰å¸§"], {
                    "default": "å‡åŒ€é‡‡æ ·",
                    "description": "å¸§é‡‡æ ·ç­–ç•¥"
                }),
                "sample_rate": ("INT", {
                    "default": 8,
                    "min": 1,
                    "max": 60,
                    "step": 1,
                    "description": "é‡‡æ ·å¸§æ•°"
                }),
                "output_format": (["çº¯æ–‡æœ¬", "JSONæ ¼å¼", "Markdownæ ¼å¼"], {
                    "default": "çº¯æ–‡æœ¬",
                    "description": "è¾“å‡ºæ ¼å¼"
                }),
                "keep_model_loaded": ("BOOLEAN", {
                    "default": False,
                    "description": "ä¿æŒæ¨¡å‹åŠ è½½ï¼ˆå‡å°‘é‡æ–°åŠ è½½æ—¶é—´ä½†å ç”¨æ›´å¤šå†…å­˜ï¼‰"
                }),
                "seed": ("INT", {
                    "default": -1,
                    "min": -1,
                    "max": 0xffffffffffffffff,
                    "description": "éšæœºç§å­ï¼Œ-1è¡¨ç¤ºéšæœº"
                }),
                "max_memory": (["æ— é™åˆ¶", "8GB", "10GB", "12GB", "16GB", "20GB", "24GB"], {
                    "default": "æ— é™åˆ¶",
                    "description": "é™åˆ¶æ¨¡å‹åœ¨ä¸åŒè®¾å¤‡ä¸Šçš„æœ€å¤§å†…å­˜ä½¿ç”¨"
                })
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING", "DICT", "INT")
    RETURN_NAMES = ("æ ¼å¼åŒ–è¾“å‡º", "è¯¦ç»†å“åº”", "å¤„ç†ä¿¡æ¯", "ç§å­")
    OUTPUT_NODE = True
    FUNCTION = "generate_video_caption"
    CATEGORY = "QwenVL-ALL"
    
    def _get_device_info(self, device: str) -> Dict[str, Any]:
        """è·å–è®¾å¤‡ä¿¡æ¯"""
        if device == "auto":
            if torch.cuda.is_available():
                device = "cuda"
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                device = "mps"
            else:
                device = "cpu"
        
        device_info = {
            "device": device,
            "device_name": torch.cuda.get_device_name() if device == "cuda" else device,
            "memory_free": torch.cuda.mem_get_info()[0] if device == "cuda" else 0,
            "memory_total": torch.cuda.mem_get_info()[1] if device == "cuda" else 0
        }
        
        return device_info
    
    def _download_model_with_progress(self, model_name: str, repo_id: str, local_dir: str):
        """ä¸‹è½½æ¨¡å‹å¹¶æ˜¾ç¤ºè¿›åº¦"""
        print(f"\n{'='*70}")
        print(f"[Qwen3-VL è§†é¢‘æè¿°] ğŸ“¥ å¼€å§‹ä¸‹è½½æ¨¡å‹: {repo_id}")
        print(f"[Qwen3-VL è§†é¢‘æè¿°] ğŸ“‚ ä¿å­˜è·¯å¾„: {local_dir}")
        print(f"[Qwen3-VL è§†é¢‘æè¿°] â³ è¯·è€å¿ƒç­‰å¾…ï¼Œä¸‹è½½å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿåˆ°å‡ ååˆ†é’Ÿ...")
        print(f"[Qwen3-VL è§†é¢‘æè¿°] ğŸ’¡ ä¸‹è½½è¿›åº¦å°†åœ¨ä¸‹æ–¹æ˜¾ç¤º")
        print(f"{'='*70}\n")

        try:
            # snapshot_download å°†è‡ªåŠ¨æ˜¾ç¤ºæ¯ä¸ªæ–‡ä»¶çš„ä¸‹è½½è¿›åº¦æ¡
            snapshot_download(
                repo_id=repo_id,
                local_dir=local_dir,
                local_dir_use_symlinks=False,
                resume_download=True,
            )
            print(f"\n{'='*70}")
            print(f"[Qwen3-VL è§†é¢‘æè¿°] âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {repo_id}")
            print(f"{'='*70}\n")
        except Exception as e:
            print(f"\n{'='*70}")
            print(f"[Qwen3-VL è§†é¢‘æè¿°] âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
            print(f"[Qwen3-VL è§†é¢‘æè¿°] ğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
            print(f"[Qwen3-VL è§†é¢‘æè¿°]    1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print(f"[Qwen3-VL è§†é¢‘æè¿°]    2. ä½¿ç”¨é•œåƒç«™ç‚¹: export HF_ENDPOINT=https://hf-mirror.com")
            print(f"[Qwen3-VL è§†é¢‘æè¿°]    3. ä½¿ç”¨ä»£ç†: export HTTP_PROXY=http://127.0.0.1:7890")
            print(f"[Qwen3-VL è§†é¢‘æè¿°]    4. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°: {local_dir}")
            print(f"{'='*70}\n")
            raise
    
    def _process_attention_type(self, attention_type: str) -> str:
        """å¤„ç†æ³¨æ„åŠ›ç±»å‹"""
        attention_map = {
            "Eageræ³¨æ„åŠ›": "eager",
            "SDPAæ³¨æ„åŠ›": "sdpa", 
            "Flashæ³¨æ„åŠ›2": "flash_attention_2"
        }
        
        for key, value in attention_map.items():
            if key in attention_type:
                return value
        
        return "sdpa"  # Default return SDPA
    
    def _parse_max_memory_option(self, option):
        """
        è§£æmax_memoryé€‰é¡¹ä¸ºå…·ä½“çš„å†…å­˜é…ç½®å­—å…¸
        
        Args:
            option (str): é€‰é¡¹åç§°
            
        Returns:
            dict or None: å†…å­˜é…ç½®å­—å…¸æˆ–None
        """
        # å®šä¹‰é¢„è®¾çš„å†…å­˜é…ç½®
        memory_configs = {
            "æ— é™åˆ¶": {},
            "8GB": {"cuda:0": "8GiB", "cpu": "16GiB"},
            "10GB": {"cuda:0": "10GiB", "cpu": "20GiB"},
            "12GB": {"cuda:0": "12GiB", "cpu": "24GiB"},
            "16GB": {"cuda:0": "16GiB", "cpu": "32GiB"},
            "20GB": {"cuda:0": "20GiB", "cpu": "40GiB"},
            "24GB": {"cuda:0": "24GiB", "cpu": "64GiB"}
        }
        
        # å¦‚æœæ˜¯é¢„è®¾é€‰é¡¹ï¼Œè¿”å›å¯¹åº”çš„é…ç½®
        if option in memory_configs:
            config = memory_configs[option]
        else:
            # ä¸æ”¯æŒè‡ªå®šä¹‰é€‰é¡¹ï¼Œè¿”å›æ— é™åˆ¶é…ç½®
            print(f"[Qwen3-VL è§†é¢‘æè¿°] ä¸æ”¯æŒçš„max_memoryé€‰é¡¹: {option}ï¼Œä½¿ç”¨æ— é™åˆ¶é…ç½®")
            return {}
        
        # å°†å­—ç¬¦ä¸²æ ¼å¼çš„å†…å­˜å¤§å°è½¬æ¢ä¸ºæ•´æ•°ï¼ˆå­—èŠ‚ï¼‰
        import re
        parsed_config = {}
        for device_id, mem_str in config.items():
            # åŒ¹é…æ•°å­—å’Œå•ä½
            match = re.match(r'^(\d+(?:\.\d+)?)([TGMK]iB|B)$', mem_str, re.IGNORECASE)
            if match:
                value, unit = match.groups()
                value = float(value)
                # è½¬æ¢ä¸ºå­—èŠ‚
                if unit.upper() == 'TB':
                    value *= 1024**4
                elif unit.upper() == 'GB':
                    value *= 1024**3
                elif unit.upper() == 'MB':
                    value *= 1024**2
                elif unit.upper() == 'KB':
                    value *= 1024
                elif unit.upper() == 'TIB':
                    value *= 1024**4
                elif unit.upper() == 'GIB':
                    value *= 1024**3
                elif unit.upper() == 'MIB':
                    value *= 1024**2
                elif unit.upper() == 'KIB':
                    value *= 1024
                parsed_config[device_id] = int(value)
        
        return parsed_config
    
    def _load_model(self, model_name: str, device: str, quantization: str, attn_implementation: str, max_memory="æ— é™åˆ¶"):
        """åŠ è½½Qwen3-VLæ¨¡å‹å’Œå¤„ç†å™¨ï¼Œæ”¯æŒç¼“å­˜"""
        # æå–å¹²å‡€çš„æ¨¡å‹åç§°ï¼ˆå»é™¤"ï¼ˆå·²ä¸‹è½½ï¼‰"æ ‡è®°ï¼‰
        clean_model_name = model_name.replace("ï¼ˆå·²ä¸‹è½½ï¼‰", "")
        
        # æ£€æŸ¥ç¼“å­˜
        if self.current_model_name == model_name and self.model_cache.get(model_name) is not None:
            return self.model_cache[model_name], self.processor_cache.get(model_name)
        
        # è·å–æ¨¡å‹æ˜ å°„
        model_repo_map = self.get_model_repo_map()
        
        # ä»æ˜ å°„ä¸­è·å–ä»“åº“IDï¼Œä½¿ç”¨å¹²å‡€çš„æ¨¡å‹åç§°
        repo_id = model_repo_map.get(clean_model_name)
        if not repo_id:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {clean_model_name}")
        
        # ä½¿ç”¨å¹²å‡€çš„æ¨¡å‹åç§°ä½œä¸ºæœ¬åœ°ç›®å½•å
        model_checkpoint = os.path.join(
            folder_paths.models_dir, "LLM", "Qwen-VL", clean_model_name
        )
        
        # å¯¹äºHuggingFaceæ¨¡å‹ï¼ˆåŒ…å«"/"ï¼‰ï¼Œéœ€è¦ä¸‹è½½
        if "/" in repo_id:
            # å¦‚æœæ¨¡å‹ä¸å­˜åœ¨åˆ™ä¸‹è½½
            if not os.path.exists(model_checkpoint) and HF_HUB_AVAILABLE:
                self._download_model_with_progress(model_name, repo_id, model_checkpoint)
            elif not os.path.exists(model_checkpoint):
                print(f"[è­¦å‘Š] æ¨¡å‹ç›®å½•ä¸å­˜åœ¨ä¸”huggingface_hubä¸å¯ç”¨ï¼Œå°†å°è¯•ç›´æ¥åŠ è½½: {model_name}")
                model_checkpoint = model_name  # å›é€€åˆ°ç›´æ¥åŠ è½½
        else:
            # å¯¹äºæœ¬åœ°æ¨¡å‹ï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨
            if not os.path.exists(model_checkpoint):
                raise ValueError(f"æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨: {model_checkpoint}")
        
        try:
            # åŠ è½½å¤„ç†å™¨
            processor = AutoProcessor.from_pretrained(
                model_checkpoint,
                trust_remote_code=True,
                local_files_only=True  # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ–‡ä»¶
            )
            
            # é…ç½®é‡åŒ–
            quantization_config = None
            if quantization == "4-bit":
                quantization_config = BitsAndBytesConfig(load_in_4bit=True)
            elif quantization == "8-bit":
                quantization_config = BitsAndBytesConfig(load_in_8bit=True)
            
            # æ£€æµ‹bf16æ”¯æŒ
            bf16_support = (
                torch.cuda.is_available()
                and torch.cuda.get_device_capability(device if device != "auto" else torch.device("cuda"))[0] >= 8
            ) if device != "cpu" else False
            
            # é€‰æ‹©æ¨¡å‹åŠ è½½ç±»
            if QWEN3VL_AVAILABLE:
                model_class = Qwen3VLForConditionalGeneration
            else:
                print("[è­¦å‘Š] Qwen3VLForConditionalGenerationä¸å¯ç”¨ï¼Œä½¿ç”¨AutoModelForVision2Seq")
                model_class = AutoModelForVision2Seq
            
            # åŠ è½½æ¨¡å‹å‚æ•°
            model_kwargs = {
                "torch_dtype": torch.bfloat16 if bf16_support else torch.float16,
                "device_map": "auto" if device == "auto" else None,
                "attn_implementation": attn_implementation,
                "quantization_config": quantization_config,
                "trust_remote_code": True,
                "low_cpu_mem_usage": True,
                "local_files_only": True  # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ–‡ä»¶
            }
            
            # é…ç½®max_memory
            max_memory_config = self._parse_max_memory_option(max_memory)
            if max_memory_config:
                model_kwargs["max_memory"] = max_memory_config
            
            # åŠ è½½æ¨¡å‹
            model = model_class.from_pretrained(
                model_checkpoint,
                **model_kwargs
            )
            
            # è®¾å¤‡ç‰¹å®šå¤„ç†
            if device != "auto" and device != "cpu" and quantization == "None":
                model = model.to(device)
            
            # ç¼“å­˜æ¨¡å‹
            self.model_cache[model_name] = model
            self.processor_cache[model_name] = processor
            self.current_model_name = model_name
            
            return model, processor
            
        except Exception as e:
            error_msg = f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
            print(f"[Qwen3-VL è§†é¢‘æè¿°] {error_msg}")
            import traceback
            traceback.print_exc()
            raise RuntimeError(error_msg)
        
        # ç¡®ä¿æ¨¡å‹æ–‡ä»¶ç¼–ç æ­£ç¡®
        import locale
        encoding = locale.getpreferredencoding()
        print(f"[Qwen3-VL è§†é¢‘æè¿°] ç³»ç»Ÿç¼–ç : {encoding}")
    
    def generate_video_caption(self, video_frames, caption_mode="è¯¦ç»†æè¿°", 
                               max_tokens=512, temperature=0.7, repetition_penalty=1.2, seed=-1, model_name="Qwen3-VL-2B-Instruct", device="auto", 
                               attention_type="SDPAæ³¨æ„åŠ›", quantization="æ— ï¼ˆFP16ï¼‰",
                               custom_prompt=None, frame_sampling="å‡åŒ€é‡‡æ ·", sample_rate=8,
                               output_format="çº¯æ–‡æœ¬", keep_model_loaded=False, max_memory="æ— é™åˆ¶") -> Tuple[str, str, Dict[str, Any]]:
        """
        ç”Ÿæˆè§†é¢‘æè¿°
        
        Args:
            video_frames: è§†é¢‘å¸§åºåˆ—
            caption_mode: æè¿°æ¨¡å¼
            max_tokens: æœ€å¤§ä»¤ç‰Œæ•°
            temperature: ç”Ÿæˆæ¸©åº¦
            seed: éšæœºç§å­ï¼Œ-1è¡¨ç¤ºéšæœº
            model_name: æ¨¡å‹åç§°
            device: è¿è¡Œæ¨¡å‹çš„è®¾å¤‡
            attention_type: æ³¨æ„åŠ›ç±»å‹
            quantization: é‡åŒ–æ¨¡å¼
            custom_prompt: è‡ªå®šä¹‰æç¤ºè¯
            frame_sampling: å¸§é‡‡æ ·ç­–ç•¥
            sample_rate: é‡‡æ ·å¸§æ•°
            output_format: è¾“å‡ºæ ¼å¼
            keep_model_loaded: ä¿æŒæ¨¡å‹åŠ è½½åœ¨å†…å­˜ä¸­
            
        Returns:
            tuple: (æ ¼å¼åŒ–è¾“å‡º, è¯¦ç»†å“åº”, å¤„ç†ä¿¡æ¯)
        """
        try:
            # å¤„ç†è§†é¢‘å¸§
            sampled_frames = self._sample_frames(video_frames, frame_sampling, sample_rate)
            
            # æ„å»ºæç¤ºè¯
            prompt = self._build_prompt(caption_mode, custom_prompt)
            
            # å¤„ç†è®¾å¤‡ä¿¡æ¯
            device_info = self._get_device_info(device)
            
            # å¤„ç†æ³¨æ„åŠ›ç±»å‹
            attn_implementation = self._process_attention_type(attention_type)
            
            # ç‹¬ç«‹åŠ è½½æ¨¡å‹
            print(f"[Qwen3-VL è§†é¢‘æè¿°] åŠ è½½æ¨¡å‹: {model_name}")
            model, processor = self._load_model(
                model_name=model_name,
                device=device_info["device"],
                quantization=quantization,
                attn_implementation=attn_implementation,
                max_memory=max_memory
            )
            
            # å°†è§†é¢‘å¸§å¤„ç†ä¸ºbase64ç¼–ç 
            image_data = []
            if sampled_frames.dim() == 4:  # (T, H, W, C)
                frames = (sampled_frames * 255).byte().cpu().numpy()
                for frame in frames:
                    # å°†å¼ é‡è½¬æ¢ä¸ºPILå›¾åƒ
                    pil_image = Image.fromarray(frame)
                    
                    # å°†PILå›¾åƒè½¬æ¢ä¸ºbase64
                    img_buffer = io.BytesIO()
                    pil_image.save(img_buffer, format='PNG')
                    img_buffer.seek(0)
                    img_b64 = base64.b64encode(img_buffer.getvalue()).decode('utf-8')
                    image_data.append(f"data:image/png;base64,{img_b64}")
            
            # æ„å»ºæ¶ˆæ¯
            messages = self._prepare_messages(prompt, image_data, None)
            
            with torch.no_grad():
                # åº”ç”¨èŠå¤©æ¨¡æ¿
                text = processor.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                
                # å¤„ç†è§†è§‰ä¿¡æ¯
                image_inputs, video_inputs, video_kwargs = process_vision_info(
                    messages,
                    return_video_kwargs=True
                )
                
                # å¦‚æœfpsæ˜¯åºåˆ—ï¼Œåˆ™ä¿®å¤video_kwargs
                if video_kwargs and 'fps' in video_kwargs:
                    fps_value = video_kwargs['fps']
                    # å¦‚æœfpsæ˜¯åºåˆ—ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
                    if isinstance(fps_value, (list, tuple)):
                        video_kwargs['fps'] = fps_value[0] if fps_value else 24
                
                # å‡†å¤‡è¾“å…¥
                inputs = processor(
                    text=[text],
                    images=image_inputs,
                    videos=video_inputs,
                    padding=True,
                    return_tensors="pt",
                    **video_kwargs
                )
                
                # è·å–è®¾å¤‡ä¿¡æ¯
                if hasattr(model, 'device'):
                    device = model.device
                else:
                    device = next(model.parameters()).device
                
                inputs = inputs.to(device)
                
                # è®¾ç½®éšæœºç§å­
                if seed != -1:
                    torch.manual_seed(seed)
                    if torch.cuda.is_available():
                        torch.cuda.manual_seed(seed)
                        torch.cuda.manual_seed_all(seed)
                
                # ç”Ÿæˆ
                generated_ids = model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    temperature=temperature,
                    repetition_penalty=repetition_penalty,
                    do_sample=True,
                    pad_token_id=processor.tokenizer.eos_token_id,
                )
                
                generated_ids_trimmed = [
                    out_ids[len(in_ids):]
                    for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
                ]
                
                result = processor.batch_decode(
                    generated_ids_trimmed,
                    skip_special_tokens=True,
                    clean_up_tokenization_spaces=False,
                )
                
                output_text = result[0] if result else ""
            
            # æ¸…ç†æ¨¡å‹ç¼“å­˜ï¼ˆä»…åœ¨ä¸ä¿æŒåŠ è½½æ—¶ï¼‰
            if not keep_model_loaded:
                if model_name in self.model_cache:
                    del self.model_cache[model_name]
                if model_name in self.processor_cache:
                    del self.processor_cache[model_name]
                self.current_model_name = None
                
                # æ¸…ç†GPUå†…å­˜
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.ipc_collect()
            
            # æ ¼å¼åŒ–è¾“å‡º
            formatted_output = self._format_output(output_text, output_format)
            
            # æ„å»ºå¤„ç†ä¿¡æ¯
            process_info = {
                "model_name": model_name,
                "caption_mode": caption_mode,
                "frame_count": len(sampled_frames),
                "total_frames": len(video_frames),
                "device": str(device),
                "attention_type": attention_type,
                "quantization": quantization,
                "temperature": temperature,
                "max_tokens": max_tokens,
                "processing_time": "N/A",  # å¯ä»¥æ·»åŠ å®é™…å¤„ç†æ—¶é—´
                "output_length": len(output_text),
                "frame_sampling": frame_sampling,
                "sample_rate": sample_rate,
                "keep_model_loaded": keep_model_loaded
            }
            
            return (formatted_output, output_text, process_info, seed)
            
        except Exception as e:
            error_msg = f"è§†é¢‘æè¿°ç”Ÿæˆå¤±è´¥: {str(e)}"
            error_info = {
                "error": error_msg,
                "model_name": model_name,
                "caption_mode": caption_mode,
                "device": device,
                "frame_sampling": frame_sampling,
                "keep_model_loaded": keep_model_loaded
            }
            return (error_msg, error_msg, error_info, seed)
    
    def _sample_frames(self, video_frames: torch.Tensor, strategy: str, rate: int) -> torch.Tensor:
        """å¸§é‡‡æ ·å¤„ç†"""
        total_frames = len(video_frames)
        
        if strategy == "All Frames":
            return video_frames
        elif strategy == "Key Frame Extraction":
            # ç®€å•çš„å…³é”®å¸§æå–é€»è¾‘ï¼ˆå¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„ç®—æ³•æ”¹è¿›ï¼‰
            indices = torch.linspace(0, total_frames-1, min(rate, total_frames), dtype=torch.long)
            return video_frames[indices]
        else:  # å‡åŒ€é‡‡æ ·
            indices = torch.linspace(0, total_frames-1, min(rate, total_frames), dtype=torch.long)
            return video_frames[indices]
    
    def _build_prompt(self, mode: str, custom_prompt: Optional[str] = None) -> str:
        """æ„å»ºæç¤ºè¯"""
        # æ ¹æ®è¯­è¨€é€‰æ‹©é»˜è®¤æç¤ºè¯
        prompts = {
            "è¯¦ç»†æè¿°": "è¯·æä¾›æ­¤è§†é¢‘å†…å®¹çš„è¯¦ç»†æè¿°ï¼ŒåŒ…æ‹¬åœºæ™¯ã€è§’è‰²ã€åŠ¨ä½œã€æƒ…æ„Ÿã€æ°›å›´ç­‰ã€‚",
            "ç®€æ´æè¿°": "è¯·ç®€æ˜æ‰¼è¦åœ°æè¿°æ­¤è§†é¢‘çš„ä¸»è¦å†…å®¹ã€‚",
            "åŠ¨ä½œåˆ†æ": "è¯·åˆ†æè§†é¢‘ä¸­çš„åŠ¨ä½œå’Œè¡Œä¸ºï¼Œæè¿°æ­£åœ¨å‘ç”Ÿçš„äº‹æƒ…ã€‚",
            "åœºæ™¯åˆ†æ": "è¯·æè¿°è§†é¢‘ä¸­çš„åœºæ™¯ã€ç¯å¢ƒå’ŒèƒŒæ™¯ã€‚",
            "æƒ…æ„Ÿåˆ†æ": "è¯·åˆ†æè§†é¢‘ä¸­è¡¨è¾¾çš„æƒ…æ„Ÿå’Œæ°›å›´ã€‚"
        }
        
        # è·å–åŸºç¡€æç¤ºè¯
        if mode == "è‡ªå®šä¹‰" and custom_prompt:
            base_prompt = custom_prompt
        else:
            base_prompt = prompts.get(mode, prompts["è¯¦ç»†æè¿°"])
        
        return base_prompt
    
    def _process_video_frames(self, frames: torch.Tensor, processor) -> List[Image.Image]:
        """å¤„ç†è§†é¢‘å¸§"""
        processed_images = []
        
        for frame in frames:
            # å°†å¼ é‡è½¬æ¢ä¸ºPILå›¾åƒ
            if isinstance(frame, torch.Tensor):
                # å‡è®¾å¸§æ˜¯[H, W, C]æ ¼å¼ï¼Œå€¼èŒƒå›´[0, 1]
                frame_np = (frame.cpu().numpy() * 255).astype(np.uint8)
                image = Image.fromarray(frame_np)
            else:
                image = frame
            
            processed_images.append(image)
        
        return processed_images
    
    def _build_messages(self, prompt: str, images: List[Image.Image]) -> List[Dict[str, Any]]:
        """æ„å»ºå¯¹è¯æ¶ˆæ¯"""
        content = [{"type": "text", "text": prompt}]
        
        # Add images
        for image in images:
            content.append({"type": "image", "image": image})
        
        messages = [
            {
                "role": "user",
                "content": content
            }
        ]
        
        return messages
    
    def _prepare_messages(self, text_prompt: str, image_data: Optional[List[str]], video_data: Optional[str]) -> List[Dict[str, Any]]:
        """å‡†å¤‡æ¶ˆæ¯å†…å®¹ï¼Œæ”¯æŒæ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘"""
        messages = []
        content = [{"type": "text", "text": text_prompt}]
        
        # æ·»åŠ å›¾åƒæ•°æ®
        if image_data:
            for img_data in image_data:
                content.append({"type": "image", "image": img_data})
        
        # æ·»åŠ è§†é¢‘æ•°æ®
        if video_data:
            content.append({"type": "video", "video": video_data})
        
        messages.append({"role": "user", "content": content})
        return messages
    
    def _format_output(self, text: str, format_type: str) -> str:
        """æ ¼å¼åŒ–è¾“å‡º"""
        if format_type == "JSON Format":
            try:
                # å°è¯•å°†æ–‡æœ¬è§£æä¸ºJSONæ ¼å¼
                structured = {
                    "description": text,
                    "summary": text[:200] + "..." if len(text) > 200 else text,
                    "length": len(text)
                }
                return json.dumps(structured, ensure_ascii=False, indent=2)
            except:
                return text
        elif format_type == "Markdown Format":
            return f"## è§†é¢‘æè¿°\n\n{text}\n\n---\n*ç”Ÿæˆæ—¶é—´: {torch.cuda.Event().elapsed_time()}ms*"
        else:
            return text


# Node mappings
NODE_CLASS_MAPPINGS = {
    "Qwen3VLVideoCaption": Qwen3VLVideoCaption
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "Qwen3VLVideoCaption": "Qwen3-VL è§†é¢‘åæ¨"
}