"""
Qwen3-VL æ–‡æœ¬ç”Ÿæˆå™¨èŠ‚ç‚¹
ç”¨äºåŸºäºæ–‡æœ¬æç¤ºç”Ÿæˆåˆ›æ„å†…å®¹
"""

import torch
import json
import os
import folder_paths
from transformers import AutoTokenizer, BitsAndBytesConfig, AutoProcessor
try:
    from transformers import Qwen3VLForConditionalGeneration
except ImportError:
    Qwen3VLForConditionalGeneration = None
from huggingface_hub import snapshot_download
import random
import re


class Qwen3VLTextGenerator:
    """Qwen3-VLæ–‡æœ¬ç”Ÿæˆå™¨ - æ”¯æŒåˆ›æ„æ–‡æœ¬ç”Ÿæˆå’Œå†…å®¹åˆ›ä½œ"""
    
    # åŸºç¡€æ¨¡å‹æ˜ å°„
    BASE_MODEL_REPO_MAP = {
        "Qwen3-VL-2B-Instruct": "Qwen/Qwen3-VL-2B-Instruct",
        "Qwen3-VL-2B-Thinking": "Qwen/Qwen3-VL-2B-Thinking",
        "Qwen3-VL-2B-Instruct-FP8": "Qwen/Qwen3-VL-2B-Instruct-FP8",
        "Qwen3-VL-2B-Thinking-FP8": "Qwen/Qwen3-VL-2B-Thinking-FP8",
        "Qwen3-VL-4B-Instruct": "Qwen/Qwen3-VL-4B-Instruct",
        "Qwen3-VL-4B-Thinking": "Qwen/Qwen3-VL-4B-Thinking",
        "Qwen3-VL-4B-Instruct-FP8": "Qwen/Qwen3-VL-4B-Instruct-FP8",
        "Qwen3-VL-4B-Thinking-FP8": "Qwen/Qwen3-VL-4B-Thinking-FP8",
        "Qwen3-VL-8B-Instruct": "Qwen/Qwen3-VL-8B-Instruct",
        "Qwen3-VL-8B-Thinking": "Qwen/Qwen3-VL-8B-Thinking",
        "Qwen3-VL-8B-Instruct-FP8": "Qwen/Qwen3-VL-8B-Instruct-FP8",
        "Qwen3-VL-8B-Thinking-FP8": "Qwen/Qwen3-VL-8B-Thinking-FP8"
    }
    
    @classmethod
    def get_available_models(cls):
        """è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨ï¼ŒåŒ…æ‹¬åŸºç¡€æ¨¡å‹å’Œæœ¬åœ°æ¨¡å‹æ–‡ä»¶å¤¹ä¸­çš„æ¨¡å‹"""
        # è·å–åŸºç¡€æ¨¡å‹åˆ—è¡¨ï¼Œå¹¶æ£€æŸ¥æœ¬åœ°æ˜¯å¦å·²ä¸‹è½½
        available_models = []
        models_dir = os.path.join(folder_paths.models_dir, "LLM", "Qwen-VL")
        
        # æ·»åŠ åŸºç¡€æ¨¡å‹ï¼Œæ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
        for model_name in cls.BASE_MODEL_REPO_MAP.keys():
            # æ£€æŸ¥æœ¬åœ°æ˜¯å¦å­˜åœ¨è¯¥æ¨¡å‹
            model_path = os.path.join(models_dir, model_name)
            if os.path.exists(model_path) and os.path.isdir(model_path):
                available_models.append(f"{model_name}ï¼ˆå·²ä¸‹è½½ï¼‰")
            else:
                available_models.append(model_name)
        
        # æ‰«ææœ¬åœ°æ¨¡å‹æ–‡ä»¶å¤¹ï¼Œæ·»åŠ ç”¨æˆ·è‡ªå®šä¹‰çš„æœ¬åœ°æ¨¡å‹
        if os.path.exists(models_dir):
            for item in os.listdir(models_dir):
                item_path = os.path.join(models_dir, item)
                # æ£€æŸ¥æ˜¯å¦ä¸ºç›®å½•
                if os.path.isdir(item_path):
                    # æå–æ¨¡å‹åç§°ï¼ˆå»é™¤"ï¼ˆå·²ä¸‹è½½ï¼‰"æ ‡è®°ï¼‰
                    clean_model_name = item.replace("ï¼ˆå·²ä¸‹è½½ï¼‰", "")
                    if clean_model_name not in cls.BASE_MODEL_REPO_MAP.keys():
                        # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨åˆ—è¡¨ä¸­ï¼ˆé¿å…é‡å¤ï¼‰
                        display_name = f"{item}ï¼ˆå·²ä¸‹è½½ï¼‰" if "ï¼ˆå·²ä¸‹è½½ï¼‰" not in item else item
                        if display_name not in available_models:
                            available_models.append(display_name)
        
        return available_models
    
    # åŠ¨æ€æ¨¡å‹æ˜ å°„ï¼Œç»“åˆåŸºç¡€æ¨¡å‹å’Œæœ¬åœ°æ¨¡å‹
    @classmethod
    def get_model_repo_map(cls):
        """è·å–æ¨¡å‹ä»“åº“æ˜ å°„ï¼ŒåŒ…æ‹¬åŸºç¡€æ¨¡å‹å’Œæœ¬åœ°æ¨¡å‹"""
        model_repo_map = cls.BASE_MODEL_REPO_MAP.copy()
        
        # æ‰«ææœ¬åœ°æ¨¡å‹æ–‡ä»¶å¤¹
        models_dir = os.path.join(folder_paths.models_dir, "LLM", "Qwen-VL")
        if os.path.exists(models_dir):
            for item in os.listdir(models_dir):
                item_path = os.path.join(models_dir, item)
                # æ£€æŸ¥æ˜¯å¦ä¸ºç›®å½•
                if os.path.isdir(item_path):
                    # æå–å¹²å‡€çš„æ¨¡å‹åç§°ï¼ˆå»é™¤"ï¼ˆå·²ä¸‹è½½ï¼‰"æ ‡è®°ï¼‰
                    clean_model_name = item.replace("ï¼ˆå·²ä¸‹è½½ï¼‰", "")
                    # å¦‚æœå¹²å‡€çš„æ¨¡å‹åç§°ä¸åœ¨æ˜ å°„ä¸­ï¼Œåˆ™æ·»åŠ åˆ°æ˜ å°„ä¸­
                    if clean_model_name not in model_repo_map:
                        # å¯¹äºæœ¬åœ°æ¨¡å‹ï¼Œä½¿ç”¨ç›®å½•åä½œä¸ºä»“åº“ID
                        model_repo_map[clean_model_name] = clean_model_name
        
        return model_repo_map
    
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "prompt": ("STRING", {
                    "default": "å†™ä¸€ä¸ªå…³äºå‹‡æ•¢éª‘å£«å’Œé­”æ³•é¾™çš„åˆ›æ„æ•…äº‹ã€‚",
                    "multiline": True,
                    "description": "è¾“å…¥æç¤ºè¯"
                }),
                "system_prompt": ("STRING", {
                    "default": "ä½ æ˜¯ä¸€ä¸ªèƒ½å¤Ÿç”Ÿæˆå¼•äººå…¥èƒœæ•…äº‹å’Œå†…å®¹çš„åˆ›æ„ä½œå®¶ã€‚",
                    "multiline": True,
                    "description": "ç³»ç»Ÿæç¤ºè¯ï¼Œç”¨äºè®¾å®šAIè§’è‰²å’Œè¡Œä¸º"
                }),
                "max_tokens": ("INT", {
                    "default": 512,
                    "min": 50,
                    "max": 2048,
                    "step": 50,
                    "description": "è¦ç”Ÿæˆçš„æœ€å¤§ä»¤ç‰Œæ•°"
                }),
                "temperature": ("FLOAT", {
                    "default": 0.7,
                    "min": 0.1,
                    "max": 2.0,
                    "step": 0.1,
                    "description": "ç”Ÿæˆæ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶è¾“å‡ºéšæœºæ€§"
                }),
                "top_p": ("FLOAT", {
                    "default": 0.9,
                    "min": 0.1,
                    "max": 1.0,
                    "step": 0.1,
                    "description": "æ ¸é‡‡æ ·å‚æ•°ï¼Œæ§åˆ¶è¾“å‡ºå¤šæ ·æ€§"
                }),
                "repetition_penalty": ("FLOAT", {
                    "default": 1.0,
                    "min": 0.1,
                    "max": 2.0,
                    "step": 0.1,
                    "description": "é‡å¤æƒ©ç½šå‚æ•°ï¼Œæ§åˆ¶é‡å¤å†…å®¹çš„ç”Ÿæˆ"
                }),
            },
            "optional": {
                "model_name": (cls.get_available_models(), {
                    "default": "Qwen3-VL-2B-Instruct",
                    "description": "é€‰æ‹©Qwen3-VLæ¨¡å‹ç‰ˆæœ¬"
                }),
                "device": (["Auto", "cuda", "cpu", "mps"], {
                    "default": "Auto",
                    "description": "è®¾å¤‡é€‰æ‹©"
                }),
                "quantization": (["æ— ï¼ˆFP16ï¼‰", "4ä½", "8ä½"], {
                    "default": "4ä½",
                    "description": "æ¨¡å‹é‡åŒ–é€‰é¡¹"
                }),
                "attention_type": (["Eager: æœ€ä½³å…¼å®¹æ€§", "SDPA: å¹³è¡¡", "Flash Attention 2: æœ€ä½³æ€§èƒ½"], {
                    "default": "Eager: æœ€ä½³å…¼å®¹æ€§",
                    "description": "æ³¨æ„åŠ›æœºåˆ¶ç±»å‹"
                }),
                "seed": ("INT", {
                    "default": -1,
                    "min": -1,
                    "max": 0xffffffffffffffff,
                    "description": "éšæœºç§å­ï¼Œ-1è¡¨ç¤ºéšæœº"
                }),
                "max_memory": (["æ— é™åˆ¶", "8GB", "10GB", "12GB", "16GB", "20GB", "24GB"], {
                    "default": "æ— é™åˆ¶",
                    "description": "é™åˆ¶æ¨¡å‹åœ¨ä¸åŒè®¾å¤‡ä¸Šçš„æœ€å¤§å†…å­˜ä½¿ç”¨"
                }),
                "clear_cache": ("BOOLEAN", {
                    "default": False,
                    "description": "æ˜¯å¦æ¸…é™¤æ¨¡å‹ç¼“å­˜"
                }),
            }
        }
    
    RETURN_TYPES = ("STRING", "STRING", "STRING", "STRING")
    RETURN_NAMES = ("ç”Ÿæˆæ–‡æœ¬", "è¯¦ç»†å“åº”", "ä½¿ç”¨ç»Ÿè®¡", "è°ƒè¯•ä¿¡æ¯")
    FUNCTION = "generate_text"
    CATEGORY = "QwenVL-ALL"
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.processor = None
        self.current_model_name = None
    
    def get_device(self, device_name):
        """è·å–è®¾å¤‡ç±»å‹"""
        if device_name == "Auto":
            if torch.cuda.is_available():
                return "cuda"
            elif torch.backends.mps.is_available():
                return "mps"
            else:
                return "cpu"
        return device_name
    
    def _parse_max_memory_option(self, max_memory_option):
        """è§£æmax_memoryé€‰é¡¹ä¸ºå®é™…çš„å†…å­˜é…ç½®"""
        memory_configs = {
            "æ— é™åˆ¶": {},
            "8GB": {"cuda:0": "8GiB", "cpu": "16GiB"},
            "10GB": {"cuda:0": "10GiB", "cpu": "20GiB"},
            "12GB": {"cuda:0": "12GiB", "cpu": "24GiB"},
            "16GB": {"cuda:0": "16GiB", "cpu": "32GiB"},
            "20GB": {"cuda:0": "20GiB", "cpu": "40GiB"},
            "24GB": {"cuda:0": "24GiB", "cpu": "48GiB"}
        }
        
        # å¦‚æœæ˜¯é¢„è®¾é€‰é¡¹ï¼Œç›´æ¥è¿”å›å¯¹åº”çš„é…ç½®
        if max_memory_option in memory_configs:
            config = memory_configs[max_memory_option]
            # è½¬æ¢å­—ç¬¦ä¸²æ ¼å¼çš„å†…å­˜å¤§å°ä¸ºå­—èŠ‚æ•°
            if config:
                parsed_config = {}
                import re
                for device_id, mem_str in config.items():
                    # åŒ¹é…æ•°å­—å’Œå•ä½
                    match = re.match(r'^(\d+(?:\.\d+)?)([TGMK]iB|B)$', mem_str, re.IGNORECASE)
                    if match:
                        value, unit = match.groups()
                        value = float(value)
                        # è½¬æ¢ä¸ºå­—èŠ‚
                        if unit.upper() == 'TB':
                            value *= 1024**4
                        elif unit.upper() == 'GB':
                            value *= 1024**3
                        elif unit.upper() == 'MB':
                            value *= 1024**2
                        elif unit.upper() == 'KB':
                            value *= 1024
                        elif unit.upper() == 'TIB':
                            value *= 1024**4
                        elif unit.upper() == 'GIB':
                            value *= 1024**3
                        elif unit.upper() == 'MIB':
                            value *= 1024**2
                        elif unit.upper() == 'KIB':
                            value *= 1024
                        parsed_config[device_id] = int(value)
                return parsed_config
            return config
        
        # ä¸æ”¯æŒè‡ªå®šä¹‰é€‰é¡¹ï¼Œè¿”å›æ— é™åˆ¶é…ç½®
        print(f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] ä¸æ”¯æŒçš„max_memoryé€‰é¡¹: {max_memory_option}ï¼Œä½¿ç”¨æ— é™åˆ¶é…ç½®")
        return {}
    
    def _download_model_with_progress(self, model_id: str, local_dir: str):
        """ä¸‹è½½æ¨¡å‹å¹¶æ˜¾ç¤ºè¿›åº¦"""
        print(f"\n{'='*70}")
        print(f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] ğŸ“¥ å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_id}")
        print(f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] ğŸ“‚ ä¿å­˜è·¯å¾„: {local_dir}")
        print(f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] â³ è¯·è€å¿ƒç­‰å¾…ï¼Œä¸‹è½½å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿåˆ°å‡ ååˆ†é’Ÿ...")
        print(f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] ğŸ’¡ ä¸‹è½½è¿›åº¦å°†åœ¨ä¸‹æ–¹æ˜¾ç¤º")
        print(f"{'='*70}\n")
        
        try:
            # snapshot_download å°†è‡ªåŠ¨æ˜¾ç¤ºæ¯ä¸ªæ–‡ä»¶çš„ä¸‹è½½è¿›åº¦æ¡
            snapshot_download(
                repo_id=model_id,
                local_dir=local_dir,
                local_dir_use_symlinks=False,
                resume_download=True,
            )
            print(f"\n{'='*70}")
            print(f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {model_id}")
            print(f"{'='*70}\n")
        except Exception as e:
            print(f"\n{'='*70}")
            print(f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
            print(f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] ğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
            print(f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ]    1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            print(f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ]    2. ä½¿ç”¨é•œåƒç«™: export HF_ENDPOINT=https://hf-mirror.com")
            print(f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ]    3. ä½¿ç”¨ä»£ç†: export HTTP_PROXY=http://127.0.0.1:7890")
            print(f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ]    4. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°: {local_dir}")
            print(f"{'='*70}\n")
            raise
    
    def load_model(self, model_name, device, attention_type, quantization, max_memory="æ— é™åˆ¶"):
        """åŠ è½½æ¨¡å‹ï¼Œæ”¯æŒç‹¬ç«‹ä¸‹è½½å’Œç¼“å­˜æœºåˆ¶"""
        # æå–å¹²å‡€çš„æ¨¡å‹åç§°ï¼ˆå»é™¤"ï¼ˆå·²ä¸‹è½½ï¼‰"æ ‡è®°ï¼‰
        clean_model_name = model_name.replace("ï¼ˆå·²ä¸‹è½½ï¼‰", "")
        
        # è®¾å¤‡æ˜ å°„
        DEVICE_MAP = {
            "Auto": "auto",
            "CPU": "cpu",
            "GPU": "cuda",
            "MPS": "mps"
        }
        
        # é‡åŒ–é…ç½®
        # é‡åŒ–é…ç½®
        quantization_config = None
        if quantization == "4ä½":
            try:
                from transformers import BitsAndBytesConfig
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.float16
                )
            except Exception as e:
                print(f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] 4-bité‡åŒ–é…ç½®å¤±è´¥: {str(e)}")
                print("[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] å›é€€åˆ°æ— é‡åŒ–")
                # å¦‚æœ4-bité‡åŒ–å¤±è´¥ï¼Œå›é€€åˆ°æ— é‡åŒ–
                quantization = "æ— ï¼ˆFP16ï¼‰"
        elif quantization == "8ä½":
            try:
                from transformers import BitsAndBytesConfig
                quantization_config = BitsAndBytesConfig(load_in_8bit=True)
            except Exception as e:
                print(f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] 8-bité‡åŒ–é…ç½®å¤±è´¥: {str(e)}")
                print("[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] å›é€€åˆ°æ— é‡åŒ–")
                quantization = "æ— ï¼ˆFP16ï¼‰"
        
        # æ³¨æ„åŠ›å®ç°é…ç½®
        ATTN_IMPLEMENTATIONS = {
            "Eager: æœ€ä½³å…¼å®¹æ€§": "eager",
            "Flash Attention 2: æ›´å¿«ä½†å…¼å®¹æ€§è¾ƒå·®": "flash_attention_2",
            "æ— ": None
        }

        # å¦‚æœè®¾å¤‡ã€æ³¨æ„åŠ›ç±»å‹æˆ–é‡åŒ–å‘ç”Ÿå˜åŒ–åˆ™é‡æ–°åŠ è½½æ¨¡å‹
        if (self.model is None or 
            self.current_device != device or 
            self.current_attention_type != attention_type or 
            self.current_quantization != quantization):
            
            # æ¸…ç†æ—§æ¨¡å‹
            if self.model is not None:
                del self.model
                self.model = None
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            
            try:
                print(f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] ğŸ”„ åˆ‡æ¢æ¨¡å‹é…ç½®...")
                print(f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] ğŸ“± å½“å‰è®¾å¤‡: {device}")
                print(f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] ğŸ” æ³¨æ„åŠ›æœºåˆ¶: {attention_type}")
                print(f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] ğŸ“‰ é‡åŒ–çº§åˆ«: {quantization}")
                
                # è·å–æ¨¡å‹æ˜ å°„
                model_repo_map = self.get_model_repo_map()
                
                # ä»æ˜ å°„ä¸­è·å–ä»“åº“IDï¼Œä½¿ç”¨å¹²å‡€çš„æ¨¡å‹åç§°
                repo_id = model_repo_map.get(clean_model_name)
                if not repo_id:
                    raise ValueError(f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] ä¸æ”¯æŒçš„æ¨¡å‹åç§°: {clean_model_name}")
                
                # ä½¿ç”¨å¹²å‡€çš„æ¨¡å‹åç§°ä½œä¸ºæœ¬åœ°ç›®å½•å
                model_checkpoint = os.path.join(folder_paths.models_dir, "LLM", "Qwen-VL", clean_model_name)
                
                # å¯¹äºHuggingFaceæ¨¡å‹ï¼ˆåŒ…å«"/"ï¼‰ï¼Œéœ€è¦ä¸‹è½½
                if "/" in repo_id:
                    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨
                    if not os.path.exists(model_checkpoint):
                        print(f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨ï¼Œå¼€å§‹ä¸‹è½½: {model_name}")
                        self._download_model_with_progress(repo_id, model_checkpoint)
                    else:
                        print(f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {model_checkpoint}")
                else:
                    # å¯¹äºæœ¬åœ°æ¨¡å‹ï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨
                    if not os.path.exists(model_checkpoint):
                        raise ValueError(f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨: {model_checkpoint}")
                
                # è·å–å®é™…è®¾å¤‡
                actual_device = DEVICE_MAP.get(device, device)
                
                # å‡†å¤‡åŠ è½½å‚æ•°
                load_kwargs = {
                    "device_map": actual_device,
                    "attn_implementation": ATTN_IMPLEMENTATIONS.get(attention_type, attention_type),
                    "torch_dtype": torch.float16 if actual_device != "cpu" else torch.float32,
                    "trust_remote_code": True
                }
                
                # é…ç½®max_memory
                max_memory_config = self._parse_max_memory_option(max_memory)
                if max_memory_config:
                    load_kwargs["max_memory"] = max_memory_config
                
                # å¦‚éœ€è¦åˆ™æ·»åŠ é‡åŒ–é…ç½®
                if quantization_config is not None:
                    load_kwargs["quantization_config"] = quantization_config
                
                print(f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] ğŸš€ åŠ è½½æ¨¡å‹...")
                self.processor = AutoProcessor.from_pretrained(model_checkpoint, trust_remote_code=True)
                if Qwen3VLForConditionalGeneration is not None:
                    self.model = Qwen3VLForConditionalGeneration.from_pretrained(model_checkpoint, **load_kwargs)
                else:
                    raise RuntimeError("[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] âŒ æ— æ³•å¯¼å…¥Qwen3VLForConditionalGenerationç±»ï¼Œè¯·æ£€æŸ¥transformersç‰ˆæœ¬")
                self.tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, trust_remote_code=True)
                    
                # æ›´æ–°å½“å‰é…ç½®
                self.current_model_name = model_name
                self.current_device = device
                self.current_attention_type = attention_type
                self.current_quantization = quantization
                
                print(f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
                
            except Exception as e:
                error_msg = f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
                print(error_msg)
                print("[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] ğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
                print("1. æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®")
                print("2. éªŒè¯è®¾å¤‡å’Œé‡åŒ–é…ç½®æ˜¯å¦æ”¯æŒ")
                print("3. æŸ¥çœ‹ä¸Šæ–¹è¯¦ç»†é”™è¯¯ä¿¡æ¯")
                raise RuntimeError(error_msg)
    
    def build_prompt(self, prompt):
        """æ„å»ºä¼˜åŒ–çš„æç¤ºè¯"""
        # å¦‚æœæ‰€æœ‰éƒ¨åˆ†éƒ½ä¸ºç©ºï¼Œæä¾›é»˜è®¤æç¤º
        if not prompt:
            return "å†™ä¸€ä¸ªå…³äºå‹‡æ•¢éª‘å£«å’Œé­”æ³•é¾™çš„åˆ›æ„æ•…äº‹ã€‚"
        
        return prompt

    def generate_text(self, prompt, system_prompt, max_tokens, temperature, top_p, 
                     repetition_penalty, seed, model_name, device, quantization="æ— ï¼ˆFP16ï¼‰", 
                     attention_type="Eager: æœ€ä½³å…¼å®¹æ€§", clear_cache=False, max_memory="æ— é™åˆ¶"):
        """ç”Ÿæˆæ–‡æœ¬ - æ”¯æŒå¤šç§åˆ›ä½œæ¨¡å¼å’Œå‚æ•°æ§åˆ¶"""
        try:
            # æ¸…ç†æ¨¡å‹ç¼“å­˜ï¼ˆå¦‚éœ€è¦ï¼‰
            if clear_cache:
                self.cleanup_model()
                print("[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] ğŸ§¹ æ¨¡å‹ç¼“å­˜å·²æ¸…ç†")
            
            # åŠ è½½æ¨¡å‹
            print(f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] ğŸš€ å‡†å¤‡åŠ è½½æ¨¡å‹: {model_name}")
            self.load_model(model_name, device, attention_type, quantization, max_memory)
            
            # ä½¿ç”¨build_promptæ–¹æ³•å¤„ç†å‚æ•°
            processed_prompt = self.build_prompt(prompt)
            
            # æ„å»ºå¯¹è¯æ¶ˆæ¯
            messages = []
            
            # å¤„ç†ç³»ç»Ÿæç¤º
            if system_prompt and system_prompt.strip():
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": processed_prompt})
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # ç¼–ç è¾“å…¥
            model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
            
            # ç”Ÿæˆå‚æ•°
            generation_kwargs = {
                "max_new_tokens": max_tokens,
                "temperature": temperature,
                "top_p": top_p,
                "repetition_penalty": repetition_penalty,
                "do_sample": True
            }
            
            print(f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] ğŸ“ å¼€å§‹æ–‡æœ¬ç”Ÿæˆ...")
            print(f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] ğŸ“¥ è¾“å…¥æç¤ºè¯: {prompt[:100]}...")
            
            # è®¾ç½®éšæœºç§å­ï¼ˆå¦‚æŒ‡å®šï¼‰
            if seed != -1:
                torch.manual_seed(seed)
                if torch.cuda.is_available():
                    torch.cuda.manual_seed(seed)
                    torch.cuda.manual_seed_all(seed)
            
            # æ‰§è¡Œç”Ÿæˆ
            generated_ids = self.model.generate(
                **model_inputs,
                **generation_kwargs
            )
            
            # å¤„ç†è¾“å‡º
            generated_ids = generated_ids[:, model_inputs['input_ids'].shape[1]:]
            response = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)
            
            # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
            input_tokens = model_inputs['input_ids'].shape[1]
            output_tokens = generated_ids.shape[1]
            total_tokens = input_tokens + output_tokens
            
            stats = {
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "total_tokens": total_tokens,
                "model_name": model_name,
                "device": device,
                "generation_params": {
                    "max_tokens": max_tokens,
                    "temperature": temperature,
                    "top_p": top_p,
                    "repetition_penalty": repetition_penalty
                }
            }
            
            debug_info = {
                "prompt": prompt,
                "system_prompt": system_prompt,
                "full_response": response,
                "messages": messages,
                "model_inputs_shape": model_inputs['input_ids'].shape
            }
            
            print(f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] âœ… æ–‡æœ¬ç”Ÿæˆå®Œæˆ!")
            print(f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] ğŸ“¤ ç”Ÿæˆé•¿åº¦: {output_tokens} ä¸ªä»¤ç‰Œ")
            
            import json
            stats_json = json.dumps(stats, ensure_ascii=False, indent=2)
            debug_info_json = json.dumps(debug_info, ensure_ascii=False, indent=2)
            
            return (response, response, stats_json, debug_info_json)
            
        except Exception as e:
            error_msg = f"[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] âŒ æ–‡æœ¬ç”Ÿæˆå¤±è´¥: {str(e)}"
            print(error_msg)
            print("[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] ğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
            print("1. æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ­£ç¡®åŠ è½½")
            print("2. éªŒè¯è¾“å…¥å‚æ•°æ˜¯å¦åˆç†")
            print("3. æŸ¥çœ‹ä¸Šæ–¹è¯¦ç»†é”™è¯¯ä¿¡æ¯")
            raise RuntimeError(error_msg)

    def cleanup_model(self):
        """æ¸…ç†æ¨¡å‹ç¼“å­˜"""
        if self.model is not None:
            del self.model
            self.model = None
        if self.processor is not None:
            del self.processor
            self.processor = None
        if self.tokenizer is not None:
            del self.tokenizer
            self.tokenizer = None
        self.current_model_name = None
        self.current_device = None
        self.current_attention_type = None
        self.current_quantization = None
        
        # æ¸…ç†GPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print("[Qwen3-VL æ–‡æœ¬ç”Ÿæˆ] ğŸ§¹ æ¨¡å‹ç¼“å­˜å·²æ¸…ç†")


# èŠ‚ç‚¹æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "Qwen3VLTextGenerator": Qwen3VLTextGenerator,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "Qwen3VLTextGenerator": "Qwen3-VL æ–‡æœ¬ç”Ÿæˆå™¨"
}